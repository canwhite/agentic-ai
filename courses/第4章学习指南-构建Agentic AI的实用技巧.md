# ç¬¬4ç« å­¦ä¹ æŒ‡å—ï¼šæ„å»ºAgentic AIçš„å®ç”¨æŠ€å·§

> **å­¦ä¹ ç›®æ ‡**ï¼šæŒæ¡è¯„ä¼°å’Œä¼˜åŒ–Agentic AIç³»ç»Ÿçš„æ ¸å¿ƒæ–¹æ³•ï¼Œå­¦ä¼šæ„å»ºå¯æµ‹é‡ã€å¯æ”¹è¿›çš„AIç³»ç»Ÿ
>
> **å‰ç½®çŸ¥è¯†**ï¼šå®Œæˆç¬¬1-3ç« ï¼Œç†Ÿæ‚‰åŸºç¡€å·¥ä½œæµã€åæ€æ¨¡å¼å’Œå·¥å…·ä½¿ç”¨
>
> **é¢„è®¡æ—¶é—´**ï¼š100-130 åˆ†é’Ÿ

---

## ç›®å½•

0. [å‰ç½®çŸ¥è¯†å›é¡¾](#0-å‰ç½®çŸ¥è¯†å›é¡¾)
1. [è¯„ä¼°ä½“ç³»æ„å»º](#1-è¯„ä¼°ä½“ç³»æ„å»º)
2. [é”™è¯¯åˆ†æä¸ä¼˜å…ˆçº§åˆ¶å®š](#2-é”™è¯¯åˆ†æä¸ä¼˜å…ˆçº§åˆ¶å®š)
3. [ç»„ä»¶çº§è¯„ä¼°å®è·µ](#3-ç»„ä»¶çº§è¯„ä¼°å®è·µ)
4. [å®æˆ˜é¡¹ç›®1ï¼šå‘ç¥¨å¤„ç†ç³»ç»Ÿä¼˜åŒ–](#4-å®æˆ˜é¡¹ç›®1å‘ç¥¨å¤„ç†ç³»ç»Ÿä¼˜åŒ–)
5. [å®æˆ˜é¡¹ç›®2ï¼šå®¢æˆ·é‚®ä»¶å›å¤è´¨é‡æå‡](#5-å®æˆ˜é¡¹ç›®2å®¢æˆ·é‚®ä»¶å›å¤è´¨é‡æå‡)
6. [å»¶è¿Ÿä¸æˆæœ¬ä¼˜åŒ–](#6-å»¶è¿Ÿä¸æˆæœ¬ä¼˜åŒ–)
7. [å¼€å‘æµç¨‹æœ€ä½³å®è·µ](#7-å¼€å‘æµç¨‹æœ€ä½³å®è·µ)
8. [å­¦ä¹ è·¯å¾„å»ºè®®](#8-å­¦ä¹ è·¯å¾„å»ºè®®)

---

## 0. å‰ç½®çŸ¥è¯†å›é¡¾

å¦‚æœä½ è·³è¿‡äº†ç¬¬1-3ç« ï¼Œæˆ–è€…éœ€è¦å¿«é€Ÿå›é¡¾æ ¸å¿ƒæ¦‚å¿µï¼Œè¿™é‡Œæ˜¯ä½ éœ€è¦äº†è§£çš„åŸºç¡€çŸ¥è¯†ï¼š

### 0.1 Agentic AI æ ¸å¿ƒæ¦‚å¿µ

**ä»€ä¹ˆæ˜¯ Agentic AIï¼Ÿ**
- **ä¼ ç»Ÿæ–¹å¼**ï¼šç”¨æˆ·ä¸€æ¬¡æ€§æé—®ï¼ŒLLM ç›´æ¥å›ç­”ï¼ˆé›¶æ ·æœ¬ï¼‰
- **Agentic æ–¹å¼**ï¼šå°†å¤æ‚ä»»åŠ¡åˆ†è§£ä¸ºå¤šä¸ªæ­¥éª¤ï¼Œæ¯ä¸ªæ­¥éª¤å¯ä»¥è°ƒç”¨å·¥å…·ã€è¿›è¡Œåæ€ã€æŸ¥è¯¢æ•°æ®ç­‰

**å…³é”®ç‰¹ç‚¹**ï¼š
- **å¤šæ­¥éª¤å·¥ä½œæµ**ï¼š3-10ä¸ªæ­¥éª¤ï¼Œè€Œä¸æ˜¯1æ­¥å®Œæˆ
- **å·¥å…·ä½¿ç”¨**ï¼šå¯ä»¥è°ƒç”¨å¤–éƒ¨å‡½æ•°å’ŒAPI
- **è‡ªæˆ‘æ”¹è¿›**ï¼šé€šè¿‡åæ€æ¨¡å¼æ£€æŸ¥å¹¶æ”¹è¿›è¾“å‡º
- **å¯è¯„ä¼°**ï¼šæ¯ä¸ªæ­¥éª¤å’Œæ•´ä½“è¾“å‡ºéƒ½å¯ä»¥æµ‹é‡è´¨é‡

### 0.2 ä¸‰å¤§æ ¸å¿ƒæ¨¡å¼å›é¡¾

| æ¨¡å¼ | æ ¸å¿ƒæ€æƒ³ | ç®€å•ç¤ºä¾‹ |
|------|----------|----------|
| **å·¥ä½œæµæ¨¡å¼** | å°†ä»»åŠ¡åˆ†è§£ä¸ºå¤šä¸ªæ­¥éª¤ | å†™è®ºæ–‡ï¼šå¤§çº² â†’ æœç´¢ â†’ å†™ä½œ â†’ æ”¹è¿› |
| **åæ€æ¨¡å¼** | è‡ªæˆ‘æ£€æŸ¥å¹¶æ”¹è¿›è¾“å‡º | ç”Ÿæˆåˆç¨¿ â†’ æ£€æŸ¥é—®é¢˜ â†’ æ”¹è¿› â†’ æœ€ç»ˆç‰ˆ |
| **å·¥å…·ä½¿ç”¨æ¨¡å¼** | è°ƒç”¨å¤–éƒ¨å‡½æ•°æ‰©å±•èƒ½åŠ› | æŸ¥è¯¢æ—¶é—´ã€æœç´¢ç½‘ç»œã€æ‰§è¡Œä»£ç  |

### 0.3 åŸºç¡€ä»£ç ç»“æ„

ä¸€ä¸ªå…¸å‹çš„ Agentic å·¥ä½œæµä»£ç ç»“æ„ï¼š
```python
def agent_workflow(input_data):
    # ç¬¬1æ­¥ï¼šé¢„å¤„ç†
    processed = preprocess(input_data)

    # ç¬¬2æ­¥ï¼šæå–ä¿¡æ¯
    extracted = extract_info(processed)

    # ç¬¬3æ­¥ï¼šæŸ¥è¯¢æ•°æ®ï¼ˆå·¥å…·ä½¿ç”¨ï¼‰
    external_data = query_database(extracted)

    # ç¬¬4æ­¥ï¼šç”Ÿæˆå“åº”
    response = generate_response(extracted, external_data)

    # ç¬¬5æ­¥ï¼šåæ€æ”¹è¿›
    improved = reflect_and_improve(response)

    return improved
```

### 0.4 å¿«é€Ÿæ£€æŸ¥ï¼šä½ å‡†å¤‡å¥½äº†å—ï¼Ÿ

å¦‚æœä½ å¯¹ä»¥ä¸‹æ¦‚å¿µæœ‰åŸºæœ¬ç†è§£ï¼Œå°±å¯ä»¥å¼€å§‹ç¬¬4ç« çš„å­¦ä¹ ï¼š
- âœ… çŸ¥é“å¦‚ä½•è°ƒç”¨ OpenAI API æˆ–å…¶ä»– LLM API
- âœ… ç†è§£ Python å‡½æ•°å’Œç±»çš„åŸºæœ¬ç”¨æ³•
- âœ… çŸ¥é“ä»€ä¹ˆæ˜¯"å¤šæ­¥éª¤å·¥ä½œæµ"
- âœ… äº†è§£"è¯„ä¼°"åœ¨è½¯ä»¶å¼€å‘ä¸­çš„é‡è¦æ€§

å¦‚æœä»¥ä¸Šæœ‰ä»»ä½•ä¸æ¸…æ¥šï¼Œå»ºè®®å…ˆå¿«é€Ÿæµè§ˆç¬¬1-3ç« çš„æ ¸å¿ƒæ¦‚å¿µéƒ¨åˆ†ã€‚

---

## 1. è¯„ä¼°ä½“ç³»æ„å»º

### 1.1 ä¸ºä»€ä¹ˆè¯„ä¼°æ˜¯æ ¸å¿ƒï¼Ÿ

**æ ¸å¿ƒç†å¿µ**ï¼šèƒ½å¦è¿›è¡Œä¸¥æ ¼è¯„ä¼°ï¼Œæ˜¯åŒºåˆ†"åšå¾—å¥½"ä¸"åšå¾—å·®"çš„æœ€å¤§é¢„æµ‹å› ç´ ã€‚

**çœŸå®æ¡ˆä¾‹å¯¹æ¯”**ï¼š
```
å›¢é˜ŸAï¼šä¸æ–­æ„å»ºæ–°åŠŸèƒ½ï¼Œä»ä¸ç³»ç»Ÿè¯„ä¼° â†’ 6ä¸ªæœˆåç³»ç»Ÿä»ç„¶ä¸ç¨³å®š
å›¢é˜ŸBï¼šæŠ•å…¥50%æ—¶é—´åšè¯„ä¼°å’Œæ”¹è¿› â†’ 3ä¸ªæœˆåç³»ç»Ÿå‡†ç¡®ç‡æå‡åˆ°92%
```

### 1.2 è¯„ä¼°åˆ†ç±»çŸ©é˜µ

| è¯„ä¼°ç±»å‹ | æœ‰æ ‡å‡†ç­”æ¡ˆ | æ— æ ‡å‡†ç­”æ¡ˆ |
|----------|------------|------------|
| **å®¢è§‚è¯„ä¼°** | ä»£ç æ‰§è¡Œã€æ•°å­¦è®¡ç®— | å“åº”æ—¶é—´ã€æˆæœ¬ |
| **ä¸»è§‚è¯„ä¼°** | æ–‡æœ¬è´¨é‡ã€ç”¨æˆ·ä½“éªŒ | åˆ›æ„æ€§ã€ç¾è§‚åº¦ |

### 1.3 å¿«é€Ÿå…¥é—¨ï¼š10åˆ†é’Ÿè¯„ä¼°ç¤ºä¾‹

åœ¨æ·±å…¥å­¦ä¹ å®Œæ•´è¯„ä¼°æ¡†æ¶å‰ï¼Œå…ˆå°è¯•è¿™ä¸ªç®€å•çš„è¯„ä¼°ç¤ºä¾‹ï¼š

åˆ›å»ºæ–‡ä»¶ `quick_evaluation.py`ï¼š

```python
# quick_evaluation.py
"""
10åˆ†é’Ÿå¿«é€Ÿä¸Šæ‰‹è¯„ä¼° - æœ€ç®€å•çš„Agentic AIè¯„ä¼°ç¤ºä¾‹
"""

def quick_evaluate_agent(test_cases):
    """
    å¿«é€Ÿè¯„ä¼°Agentçš„ç®€å•å‡½æ•°
    """
    results = {
        "total": len(test_cases),
        "correct": 0,
        "incorrect": 0,
        "accuracy": 0.0
    }

    print("ğŸ” å¼€å§‹å¿«é€Ÿè¯„ä¼°...")
    print("-" * 40)

    for i, case in enumerate(test_cases, 1):
        question = case["question"]
        expected = case["expected"]

        # æ¨¡æ‹ŸAgentå›ç­”ï¼ˆå®é™…ä¸­è¿™é‡Œä¼šè°ƒç”¨ä½ çš„Agentï¼‰
        # è¿™é‡Œç”¨ç®€å•çš„å­—ç¬¦ä¸²åŒ¹é…æ¨¡æ‹Ÿ
        if "åŠ æ³•" in question or "+" in question:
            answer = "4"  # æ¨¡æ‹Ÿå›ç­”
        elif "æ—¶é—´" in question:
            answer = "ç°åœ¨æ˜¯ä¸‹åˆ3ç‚¹"
        else:
            answer = "æˆ‘ä¸çŸ¥é“"

        # ç®€å•è¯„ä¼°
        is_correct = (answer == expected)

        if is_correct:
            results["correct"] += 1
            status = "âœ…"
        else:
            results["incorrect"] += 1
            status = "âŒ"

        print(f"{status} æµ‹è¯• {i}: {question}")
        print(f"   é¢„æœŸ: {expected}, å®é™…: {answer}")

    # è®¡ç®—å‡†ç¡®ç‡
    results["accuracy"] = results["correct"] / results["total"] if results["total"] > 0 else 0

    print("-" * 40)
    print(f"ğŸ“Š è¯„ä¼°ç»“æœ:")
    print(f"   æ€»æ•°: {results['total']}")
    print(f"   æ­£ç¡®: {results['correct']}")
    print(f"   é”™è¯¯: {results['incorrect']}")
    print(f"   å‡†ç¡®ç‡: {results['accuracy']:.2%}")

    return results

# è¿è¡Œå¿«é€Ÿè¯„ä¼°
if __name__ == "__main__":
    # ç®€å•çš„æµ‹è¯•æ¡ˆä¾‹
    test_cases = [
        {"question": "2+2ç­‰äºå‡ ï¼Ÿ", "expected": "4"},
        {"question": "ç°åœ¨å‡ ç‚¹ï¼Ÿ", "expected": "ç°åœ¨æ˜¯ä¸‹åˆ3ç‚¹"},
        {"question": "åŒ—äº¬å¤©æ°”å¦‚ä½•ï¼Ÿ", "expected": "æ™´å¤©"}
    ]

    results = quick_evaluate_agent(test_cases)

    print("\nğŸ’¡ å¿«é€Ÿè¯„ä¼°å®Œæˆï¼")
    print("ä¸‹ä¸€æ­¥ï¼š")
    print("1. å°†æ¨¡æ‹Ÿå›ç­”æ›¿æ¢ä¸ºä½ çš„çœŸå®Agent")
    print("2. å¢åŠ æ›´å¤šæµ‹è¯•æ¡ˆä¾‹")
    print("3. ä½¿ç”¨ä¸‹é¢çš„å®Œæ•´è¯„ä¼°æ¡†æ¶")
```

**è¿è¡Œè¿™ä¸ªå¿«é€Ÿç¤ºä¾‹**ï¼š
```bash
python quick_evaluation.py
```

**è¾“å‡ºç¤ºä¾‹**ï¼š
```
ğŸ” å¼€å§‹å¿«é€Ÿè¯„ä¼°...
----------------------------------------
âœ… æµ‹è¯• 1: 2+2ç­‰äºå‡ ï¼Ÿ
   é¢„æœŸ: 4, å®é™…: 4
âŒ æµ‹è¯• 2: ç°åœ¨å‡ ç‚¹ï¼Ÿ
   é¢„æœŸ: ç°åœ¨æ˜¯ä¸‹åˆ3ç‚¹, å®é™…: ç°åœ¨æ˜¯ä¸‹åˆ3ç‚¹
âŒ æµ‹è¯• 3: åŒ—äº¬å¤©æ°”å¦‚ä½•ï¼Ÿ
   é¢„æœŸ: æ™´å¤©, å®é™…: æˆ‘ä¸çŸ¥é“
----------------------------------------
ğŸ“Š è¯„ä¼°ç»“æœ:
   æ€»æ•°: 3
   æ­£ç¡®: 1
   é”™è¯¯: 2
   å‡†ç¡®ç‡: 33.33%

ğŸ’¡ å¿«é€Ÿè¯„ä¼°å®Œæˆï¼
ä¸‹ä¸€æ­¥ï¼š
1. å°†æ¨¡æ‹Ÿå›ç­”æ›¿æ¢ä¸ºä½ çš„çœŸå®Agent
2. å¢åŠ æ›´å¤šæµ‹è¯•æ¡ˆä¾‹
3. ä½¿ç”¨ä¸‹é¢çš„å®Œæ•´è¯„ä¼°æ¡†æ¶
```

### 1.4 åŸºç¡€è¯„ä¼°æ¡†æ¶

åˆ›å»ºæ–‡ä»¶ `evaluation_framework.py`ï¼š

```python
# evaluation_framework.py
import json
import time
from typing import Dict, List, Any
from datetime import datetime

class AgenticEvaluator:
    def __init__(self):
        self.evaluation_history = []
        self.metrics_cache = {}

    def objective_evaluation(self, test_cases: List[Dict]) -> Dict:
        """
        å®¢è§‚è¯„ä¼° - æœ‰æ˜ç¡®å¯¹é”™æ ‡å‡†
        """
        results = {
            "total_cases": len(test_cases),
            "passed_cases": 0,
            "failed_cases": 0,
            "accuracy": 0.0,
            "details": []
        }

        for case in test_cases:
            try:
                start_time = time.time()
                # æ‰§è¡ŒAgentå·¥ä½œæµ
                output = self.run_agent_workflow(case["input"])

                # å¯¹æ¯”é¢„æœŸç»“æœ
                is_correct = self.check_correctness(output, case["expected"])

                results["details"].append({
                    "case_id": case.get("id"),
                    "input": case["input"],
                    "expected": case["expected"],
                    "output": output,
                    "correct": is_correct,
                    "execution_time": time.time() - start_time
                })

                if is_correct:
                    results["passed_cases"] += 1
                else:
                    results["failed_cases"] += 1

            except Exception as e:
                results["details"].append({
                    "case_id": case.get("id"),
                    "error": str(e),
                    "correct": False
                })
                results["failed_cases"] += 1

        results["accuracy"] = results["passed_cases"] / results["total_cases"]
        return results

    def check_correctness(self, output: any, expected: any) -> bool:
        """å¯¹æ¯”è¾“å‡ºå’Œé¢„æœŸç»“æœ"""
        # æ•°å€¼æ¯”è¾ƒï¼ˆå…è®¸å°è¯¯å·®ï¼‰
        if isinstance(expected, (int, float)):
            try:
                return abs(float(output) - expected) < 0.01
            except (ValueError, TypeError):
                return False

        # å­—ç¬¦ä¸²æ¯”è¾ƒï¼ˆå¿½ç•¥å¤§å°å†™å’Œç©ºæ ¼ï¼‰
        if isinstance(expected, str):
            return str(output).strip().lower() == str(expected).strip().lower()

        return output == expected

    def run_agent_workflow(self, input_data: str) -> str:
        """è¿è¡ŒAgentå·¥ä½œæµï¼ˆç¤ºä¾‹ï¼‰"""
        # è¿™é‡Œåº”è¯¥æ˜¯å®é™…çš„Agentå·¥ä½œæµ
        # ç®€åŒ–ç¤ºä¾‹
        return f"å¤„ç†ç»“æœï¼š{input_data}"

# ä½¿ç”¨ç¤ºä¾‹
def basic_evaluation_demo():
    """åŸºç¡€è¯„ä¼°æ¼”ç¤º"""
    evaluator = AgenticEvaluator()

    # å®¢è§‚è¯„ä¼°ç¤ºä¾‹ - æ•°å­¦è®¡ç®—
    math_test_cases = [
        {"id": 1, "input": "è®¡ç®— 2 + 2", "expected": "4"},
        {"id": 2, "input": "è®¡ç®— 15 * 8", "expected": "120"}
    ]

    print("=== å®¢è§‚è¯„ä¼°ï¼šæ•°å­¦è®¡ç®— ===")
    obj_results = evaluator.objective_evaluation(math_test_cases)
    print(f"å‡†ç¡®ç‡ï¼š{obj_results['accuracy']:.2%}")
    print(f"é€šè¿‡ï¼š{obj_results['passed_cases']}, å¤±è´¥ï¼š{obj_results['failed_cases']}")

if __name__ == "__main__":
    basic_evaluation_demo()
```

### 1.5 è¯„ä¼°æ•°æ®é›†æ„å»º

åˆ›å»ºæ–‡ä»¶ `evaluation_datasets.py`ï¼š

```python
# evaluation_datasets.py

# å‘ç¥¨å¤„ç†è¯„ä¼°æ•°æ®é›†
INVOICE_EVALUATION_DATASET = [
    {
        "id": "invoice_001",
        "input": """
        å‘ç¥¨å·ç ï¼šINV-2024-001
        å¼€ç¥¨æ—¥æœŸï¼š2024å¹´1æœˆ15æ—¥
        åˆ°æœŸæ—¥æœŸï¼š2024å¹´2æœˆ15æ—¥
        å¼€ç¥¨æ–¹ï¼šABCç§‘æŠ€æœ‰é™å…¬å¸
        åº”ä»˜é‡‘é¢ï¼šï¿¥5,000.00
        é¡¹ç›®ï¼šè½¯ä»¶å¼€å‘æœåŠ¡
        """,
        "expected": {
            "invoice_number": "INV-2024-001",
            "issue_date": "2024-01-15",
            "due_date": "2024-02-15",
            "biller_name": "ABCç§‘æŠ€æœ‰é™å…¬å¸",
            "amount": 5000.00,
            "project_description": "è½¯ä»¶å¼€å‘æœåŠ¡"
        }
    }
]

# å®¢æœé‚®ä»¶è¯„ä¼°æ•°æ®é›†
CUSTOMER_SERVICE_EVALUATION_DATASET = [
    {
        "id": "cs_001",
        "input": "æˆ‘è®¢è´­äº†è“è‰²æ…æ‹Œæœºï¼Œæ”¶åˆ°çº¢è‰²çƒ¤é¢åŒ…æœº",
        "expected_keywords": ["é“æ­‰", "è§£å†³", "è®¢å•"],
        "expected_emotion": "understanding"
    }
]
```

---

## 2. é”™è¯¯åˆ†æä¸ä¼˜å…ˆçº§åˆ¶å®š

### 2.1 é”™è¯¯åˆ†ææ¡†æ¶

åˆ›å»ºæ–‡ä»¶ `error_analysis.py`ï¼š

```python
# error_analysis.py
import pandas as pd
from collections import Counter
from typing import Dict, List, Any

class ErrorAnalyzer:
    def __init__(self):
        self.error_categories = {
            "input_parsing": "è¾“å…¥è§£æé”™è¯¯",
            "tool_execution": "å·¥å…·æ‰§è¡Œé”™è¯¯",
            "llm_reasoning": "LLMæ¨ç†é”™è¯¯",
            "output_format": "è¾“å‡ºæ ¼å¼é”™è¯¯",
            "external_api": "å¤–éƒ¨APIé”™è¯¯"
        }

    def analyze_errors(self, execution_traces: List[Dict]) -> Dict:
        """ç³»ç»ŸåŒ–é”™è¯¯åˆ†æ"""
        analysis = {
            "total_cases": len(execution_traces),
            "error_distribution": {},
            "error_patterns": [],
            "recommendations": [],
            "detailed_analysis": {}
        }

        # åˆ†ç±»ç»Ÿè®¡é”™è¯¯
        error_counts = Counter()
        error_details = []

        for trace in execution_traces:
            if trace.get("has_error", False):
                error_type = self.classify_error(trace["error"])
                error_counts[error_type] += 1

                error_details.append({
                    "case_id": trace.get("case_id"),
                    "error_type": error_type,
                    "error_message": trace["error"]["message"],
                    "step_where_failed": trace["error"].get("failed_step"),
                    "context": trace.get("context", {})
                })

        # ç”Ÿæˆé”™è¯¯åˆ†å¸ƒ
        analysis["error_distribution"] = dict(error_counts)

        # è¯†åˆ«é”™è¯¯æ¨¡å¼
        analysis["error_patterns"] = self.identify_patterns(error_details)

        # ç”Ÿæˆæ”¹è¿›å»ºè®®
        analysis["recommendations"] = self.generate_recommendations(analysis["error_patterns"])

        return analysis

    def classify_error(self, error: Dict) -> str:
        """åˆ†ç±»é”™è¯¯ç±»å‹"""
        error_message = error.get("message", "").lower()

        if any(word in error_message for word in ["parse", "format", "invalid"]):
            return "input_parsing"
        elif any(word in error_message for word in ["tool", "function", "execution"]):
            return "tool_execution"
        elif any(word in error_message for word in ["llm", "reasoning", "understand"]):
            return "llm_reasoning"
        elif any(word in error_message for word in ["output", "format", "structure"]):
            return "output_format"
        elif any(word in error_message for word in ["api", "network", "connection"]):
            return "external_api"
        else:
            return "unknown"

    def create_priority_matrix(self, analysis: Dict) -> pd.DataFrame:
        """åˆ›å»ºä¼˜å…ˆçº§çŸ©é˜µ"""
        # åŸºäºé”™è¯¯é¢‘ç‡å’Œå½±å“ç¨‹åº¦åˆ›å»ºä¼˜å…ˆçº§çŸ©é˜µ
        error_types = list(analysis["error_distribution"].keys())

        # æ¨¡æ‹Ÿå½±å“ç¨‹åº¦ï¼ˆå®é™…åº”è¯¥åŸºäºä¸šåŠ¡å½±å“ï¼‰
        impact_scores = {
            "input_parsing": 8,      # é«˜å½±å“
            "tool_execution": 9,     # é«˜å½±å“
            "llm_reasoning": 6,      # ä¸­ç­‰å½±å“
            "output_format": 4,      # ä½å½±å“
            "external_api": 7        # ä¸­é«˜å½±å“
        }

        priority_data = []
        for error_type in error_types:
            frequency = analysis["error_distribution"][error_type]
            impact = impact_scores.get(error_type, 5)
            priority = frequency * impact  # ç®€å•ä¼˜å…ˆçº§è®¡ç®—

            priority_data.append({
                "é”™è¯¯ç±»å‹": self.error_categories[error_type],
                "å‘ç”Ÿé¢‘ç‡": frequency,
                "ä¸šåŠ¡å½±å“": impact,
                "ä¼˜å…ˆçº§å¾—åˆ†": priority,
                "å»ºè®®å¤„ç†é¡ºåº": 0
            })

        # æ’åºå¹¶æ·»åŠ é¡ºåº
        priority_df = pd.DataFrame(priority_data)
        priority_df = priority_df.sort_values("ä¼˜å…ˆçº§å¾—åˆ†", ascending=False)
        priority_df["å»ºè®®å¤„ç†é¡ºåº"] = range(1, len(priority_df) + 1)

        return priority_df

# ä½¿ç”¨ç¤ºä¾‹
def error_analysis_demo():
    """é”™è¯¯åˆ†ææ¼”ç¤º"""
    analyzer = ErrorAnalyzer()

    # æ¨¡æ‹Ÿæ‰§è¡Œè¿½è¸ªæ•°æ®
    execution_traces = [
        {
            "case_id": 1,
            "has_error": True,
            "error": {
                "message": "Failed to parse invoice date format",
                "failed_step": "date_extraction"
            }
        }
    ]

    print("=== é”™è¯¯åˆ†ææ¼”ç¤º ===")
    analysis = analyzer.analyze_errors(execution_traces)

    print(f"é”™è¯¯ç‡ï¼š{analysis['detailed_analysis']['error_rate']:.2%}")
    print(f"é”™è¯¯åˆ†å¸ƒï¼š{analysis['error_distribution']}")

    # ç”Ÿæˆä¼˜å…ˆçº§çŸ©é˜µ
    priority_matrix = analyzer.create_priority_matrix(analysis)
    print("\nä¼˜å…ˆçº§çŸ©é˜µï¼š")
    print(priority_matrix.to_string(index=False))

if __name__ == "__main__":
    error_analysis_demo()
```

---

## 3. ç»„ä»¶çº§è¯„ä¼°å®è·µ

### 3.1 ç ”ç©¶æœç´¢ç»„ä»¶è¯„ä¼°

åˆ›å»ºæ–‡ä»¶ `research_component_eval.py`ï¼š

```python
# research_component_eval.py
import requests
import json
from typing import List, Dict

class ResearchComponentEvaluator:
    def __init__(self):
        self.gold_standard_domains = [
            "arxiv.org", "ieee.org", "acm.org",  # å­¦æœ¯æ¥æº
            "microsoft.com", "google.com", "openai.com",  # æŠ€æœ¯å…¬å¸
            "wikipedia.org", "britannica.com"  # ç™¾ç§‘
        ]

    def evaluate_search_component(self, test_queries: List[str]) -> Dict:
        """è¯„ä¼°ç ”ç©¶æœç´¢ç»„ä»¶"""
        results = {
            "queries_evaluated": len(test_queries),
            "average_precision": 0.0,
            "average_recall": 0.0,
            "average_f1": 0.0,
            "domain_accuracy": 0.0,
            "detailed_results": []
        }

        precisions = []
        recalls = []
        f1_scores = []
        domain_correct = 0

        for query in test_queries:
            # æ‰§è¡Œæœç´¢
            search_results = self.execute_search(query)

            # è¯„ä¼°æœç´¢ç»“æœ
            evaluation = self.evaluate_search_quality(search_results, query)

            precisions.append(evaluation["precision"])
            recalls.append(evaluation["recall"])
            f1_scores.append(evaluation["f1"])

            if evaluation["has_reputable_domain"]:
                domain_correct += 1

            results["detailed_results"].append({
                "query": query,
                "num_results": len(search_results),
                "precision": evaluation["precision"],
                "recall": evaluation["recall"],
                "f1": evaluation["f1"],
                "reputable_domains": evaluation["reputable_domains"]
            })

        # è®¡ç®—å¹³å‡å€¼
        if precisions:
            results["average_precision"] = sum(precisions) / len(precisions)
            results["average_recall"] = sum(recalls) / len(recalls)
            results["average_f1"] = sum(f1_scores) / len(f1_scores)

        results["domain_accuracy"] = domain_correct / len(test_queries)

        return results

    def execute_search(self, query: str) -> List[Dict]:
        """æ‰§è¡Œæœç´¢ï¼ˆæ¨¡æ‹Ÿå®ç°ï¼‰"""
        # æ¨¡æ‹Ÿè¿”å›ç»“æœ
        mock_results = [
            {
                "title": f"å…³äº{query}çš„ç ”ç©¶",
                "url": "https://arxiv.org/abs/1234",
                "snippet": f"è¿™ç¯‡è®ºæ–‡è®¨è®ºäº†{query}çš„é‡è¦æ¦‚å¿µ..."
            },
            {
                "title": f"{query}æŠ€æœ¯æ–‡æ¡£",
                "url": "https://microsoft.com/research/ai",
                "snippet": f"Microsoft Researchçš„æœ€æ–°è¿›å±•åŒ…æ‹¬{query}..."
            }
        ]

        return mock_results[:5]

    def evaluate_search_quality(self, results: List[Dict], query: str) -> Dict:
        """è¯„ä¼°æœç´¢è´¨é‡"""
        # è®¡ç®—Precisionï¼ˆå‡è®¾å‰3ä¸ªç»“æœæ˜¯ç›¸å…³çš„ï¼‰
        relevant_results = min(len(results), 3)
        precision = relevant_results / len(results) if results else 0

        # è®¡ç®—Recallï¼ˆå‡è®¾æ€»å…±æœ‰5ä¸ªç›¸å…³æ–‡æ¡£ï¼‰
        recall = relevant_results / 5

        # è®¡ç®—F1åˆ†æ•°
        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0

        # æ£€æŸ¥æ˜¯å¦æœ‰æƒå¨åŸŸå
        reputable_domains = []
        for result in results:
            domain = self.extract_domain(result["url"])
            if domain in self.gold_standard_domains:
                reputable_domains.append(domain)

        return {
            "precision": precision,
            "recall": recall,
            "f1": f1,
            "reputable_domains": reputable_domains,
            "has_reputable_domain": len(reputable_domains) > 0
        }

    def extract_domain(self, url: str) -> str:
        """æå–åŸŸå"""
        from urllib.parse import urlparse
        parsed = urlparse(url)
        return parsed.netloc.replace("www.", "")

# é«˜çº§è¯„ä¼°ï¼šå¯¹æŠ—æ€§æµ‹è¯•
class AdversarialEvaluator:
    def __init__(self):
        self.perturbation_strategies = [
            self.add_typos,
            self.change_formatting,
            self.add_irrelevant_info
        ]

    def generate_adversarial_examples(self, original_query: str) -> List[str]:
        """ç”Ÿæˆå¯¹æŠ—æ€§ç¤ºä¾‹"""
        adversarial_examples = []

        for strategy in self.perturbation_strategies:
            perturbed = strategy(original_query)
            if perturbed != original_query:
                adversarial_examples.append(perturbed)

        return adversarial_examples

    def add_typos(self, text: str) -> str:
        """æ·»åŠ æ‹¼å†™é”™è¯¯"""
        words = text.split()
        if len(words) > 3:
            import random
            idx = random.randint(0, len(words) - 1)
            word = words[idx]
            if len(word) > 3:
                words[idx] = word[:-1] + word[-2] + word[-1]
        return " ".join(words)

    def evaluate_robustness(self, original_query: str, component_func) -> Dict:
        """è¯„ä¼°ç»„ä»¶é²æ£’æ€§"""
        adversarial_examples = self.generate_adversarial_examples(original_query)

        results = {
            "original_query": original_query,
            "total_adversarial": len(adversarial_examples),
            "successful_handling": 0,
            "robustness_score": 0.0
        }

        # æµ‹è¯•åŸå§‹æŸ¥è¯¢
        original_result = component_func(original_query)

        for adversarial_query in adversarial_examples:
            try:
                result = component_func(adversarial_query)

                if self.is_successful_handling(original_result, result):
                    results["successful_handling"] += 1

            except Exception as e:
                pass  # å¤„ç†å¤±è´¥çš„æƒ…å†µ

        results["robustness_score"] = results["successful_handling"] / results["total_adversarial"]

        return results

    def is_successful_handling(self, original_result: any, adversarial_result: any) -> bool:
        """åˆ¤æ–­æ˜¯å¦æˆåŠŸå¤„ç†å¯¹æŠ—æ€§æŸ¥è¯¢"""
        return (adversarial_result is not None and
                type(adversarial_result) == type(original_result))

# ä½¿ç”¨ç¤ºä¾‹
def component_evaluation_demo():
    """ç»„ä»¶è¯„ä¼°æ¼”ç¤º"""
    evaluator = ResearchComponentEvaluator()

    # æµ‹è¯•æŸ¥è¯¢
    test_queries = [
        "artificial intelligence",
        "machine learning applications"
    ]

    print("=== ç ”ç©¶æœç´¢ç»„ä»¶è¯„ä¼° ===")
    results = evaluator.evaluate_search_component(test_queries)

    print(f"å¹³å‡Precisionï¼š{results['average_precision']:.3f}")
    print(f"å¹³å‡F1ï¼š{results['average_f1']:.3f}")
    print(f"æƒå¨åŸŸåå‡†ç¡®ç‡ï¼š{results['domain_accuracy']:.3f}")

    # å¯¹æŠ—æ€§æµ‹è¯•
    print("\n=== å¯¹æŠ—æ€§é²æ£’æ€§æµ‹è¯• ===")
    adversarial_eval = AdversarialEvaluator()

    original_query = "å®¢æˆ·è®¢å•æŸ¥è¯¢"

    def mock_search_component(query):
        # æ¨¡æ‹Ÿæœç´¢ç»„ä»¶
        if "è®¢å•" in query:
            return {"results": ["è®¢å•1", "è®¢å•2"], "count": 2}
        return {"results": [], "count": 0}

    robustness_results = adversarial_eval.evaluate_robustness(
        original_query, mock_search_component
    )

    print(f"é²æ£’æ€§å¾—åˆ†ï¼š{robustness_results['robustness_score']:.3f}")

if __name__ == "__main__":
    component_evaluation_demo()
```

---

## 4. å®æˆ˜é¡¹ç›®1ï¼šå‘ç¥¨å¤„ç†ç³»ç»Ÿä¼˜åŒ–

### 4.1 é¡¹ç›®æ¦‚è¿°

**ç›®æ ‡**ï¼šæ„å»ºä¸€ä¸ªé«˜å‡†ç¡®åº¦çš„å‘ç¥¨ä¿¡æ¯æå–ç³»ç»Ÿï¼Œå¹¶é€šè¿‡ç»„ä»¶çº§è¯„ä¼°æŒç»­ä¼˜åŒ–ã€‚

**æŒ‘æˆ˜**ï¼š
- å¤„ç†ä¸åŒæ ¼å¼çš„å‘ç¥¨
- ç¡®ä¿æ—¥æœŸã€é‡‘é¢ã€å¼€ç¥¨æ–¹ç­‰å…³é”®ä¿¡æ¯æå–å‡†ç¡®
- è¯†åˆ«å’Œçº æ­£æå–é”™è¯¯
- æŒç»­ä¼˜åŒ–ç³»ç»Ÿæ€§èƒ½

### 4.2 å®Œæ•´é¡¹ç›®å®ç°

åˆ›å»ºé¡¹ç›®ç»“æ„ï¼š
```
invoice_optimization_project/
â”œâ”€â”€ config.py              # é…ç½®
â”œâ”€â”€ invoice_extractor.py   # æ ¸å¿ƒæå–å™¨
â”œâ”€â”€ component_evaluator.py # ç»„ä»¶è¯„ä¼°
â”œâ”€â”€ error_analyzer.py      # é”™è¯¯åˆ†æ
â”œâ”€â”€ optimization_engine.py # ä¼˜åŒ–å¼•æ“
â””â”€â”€ main.py               # ä¸»ç¨‹åº
```

#### invoice_extractor.py - æ ¸å¿ƒå‘ç¥¨æå–å™¨
```python
# invoice_extractor.py
import re
import json
from datetime import datetime
from typing import Dict, List, Optional, Tuple
from openai import OpenAI

class InvoiceExtractor:
    def __init__(self):
        self.client = OpenAI()
        self.extraction_history = []
        self.field_patterns = {
            "invoice_number": [
                r'å‘ç¥¨å·ç ?[ï¼š:]?\s*([A-Z]{2,5}-\d{4,10})',
                r'å‘ç¥¨ä»£ç ?[ï¼š:]?\s*(\d{8,12})',
                r'No\.?\s*[ï¼š:]?\s*([A-Z0-9-]{6,20})'
            ],
            "date": r'(\d{4})[-å¹´/](\d{1,2})[-æœˆ/](\d{1,2})[æ—¥å·]?',
            "amount": r'ä»·ç¨åˆè®¡[ï¼š:]?\s*ï¿¥?([\d,]+\.\d{2})',
            "company": r'å¼€ç¥¨æ–¹[ï¼š:]?\s*([^\d\n]{4,50})'
        }

    def extract_invoice_info(self, invoice_text: str, use_component_evaluation: bool = True) -> Dict:
        """æå–å‘ç¥¨ä¿¡æ¯ï¼ˆä¸»æ–¹æ³•ï¼‰"""
        print(f"ğŸ” å¼€å§‹æå–å‘ç¥¨ä¿¡æ¯...")

        # ç¬¬1æ­¥ï¼šé¢„å¤„ç†
        cleaned_text = self.preprocess_invoice_text(invoice_text)

        # ç¬¬2æ­¥ï¼šå­—æ®µæå–ï¼ˆä½¿ç”¨ç»„ä»¶çº§è¯„ä¼°ï¼‰
        if use_component_evaluation:
            extraction_result = self.extract_with_component_evaluation(cleaned_text)
        else:
            extraction_result = self.basic_extraction(cleaned_text)

        # ç¬¬3æ­¥ï¼šåå¤„ç†å’ŒéªŒè¯
        validated_result = self.validate_extraction(extraction_result)

        # ç¬¬4æ­¥ï¼šè®°å½•å†å²
        self.extraction_history.append({
            "timestamp": datetime.now().isoformat(),
            "original_text": invoice_text[:200],
            "extraction_result": validated_result,
            "confidence": validated_result.get("confidence", 0)
        })

        return validated_result

    def extract_with_component_evaluation(self, text: str) -> Dict:
        """ä½¿ç”¨ç»„ä»¶çº§è¯„ä¼°è¿›è¡Œæå–"""
        results = {}
        confidence_scores = {}

        # é€å­—æ®µæå–å’Œè¯„ä¼°
        for field_name in ["invoice_number", "issue_date", "amount", "biller_name"]:
            print(f"  ğŸ“‹ æå–å­—æ®µ: {field_name}")

            if field_name == "invoice_number":
                value, confidence = self.extract_invoice_number(text)
            elif field_name.endswith("_date"):
                value, confidence = self.extract_date(text, field_name)
            elif field_name == "amount":
                value, confidence = self.extract_amount(text)
            elif field_name == "biller_name":
                value, confidence = self.extract_biller_name(text)

            results[field_name] = value
            confidence_scores[field_name] = confidence
            print(f"    ç»“æœ: {value} (ç½®ä¿¡åº¦: {confidence:.2f})")

        # è®¡ç®—æ•´ä½“ç½®ä¿¡åº¦
        overall_confidence = sum(confidence_scores.values()) / len(confidence_scores)
        results["confidence"] = overall_confidence
        results["field_confidences"] = confidence_scores

        return results

    def extract_invoice_number(self, text: str) -> Tuple[str, float]:
        """æå–å‘ç¥¨å·ç """
        patterns = self.field_patterns["invoice_number"]

        for pattern in patterns:
            match = re.search(pattern, text, re.IGNORECASE)
            if match:
                invoice_num = match.group(1).strip()
                if self.validate_invoice_number_format(invoice_num):
                    return invoice_num, 0.95

        # å›é€€åˆ°LLMæå–
        return self.extract_with_llm(text, "invoice_number")

    def extract_date(self, text: str, date_type: str) -> Tuple[str, float]:
        """æå–æ—¥æœŸ"""
        pattern = self.field_patterns["date"]
        match = re.search(pattern, text)

        if match:
            date_str = f"{match.group(1)}-{match.group(2)}-{match.group(3)}"
            if self.validate_date(date_str):
                return date_str, 0.9

        # å›é€€åˆ°LLMæå–
        return self.extract_with_llm(text, date_type)

    def extract_amount(self, text: str) -> Tuple[float, float]:
        """æå–é‡‘é¢"""
        pattern = self.field_patterns["amount"]
        match = re.search(pattern, text)

        if match:
            amount_str = match.group(1).replace(",", "")
            try:
                amount = float(amount_str)
                if amount > 0:  # éªŒè¯åˆç†æ€§
                    return amount, 0.95
            except ValueError:
                pass

        # å›é€€åˆ°LLMæå–
        return self.extract_with_llm(text, "amount")

    def extract_with_llm(self, text: str, field_name: str) -> Tuple[str, float]:
        """ä½¿ç”¨LLMæå–å­—æ®µ"""
        prompt = f"""
        è¯·ä»ä»¥ä¸‹å‘ç¥¨æ–‡æœ¬ä¸­æå–"{field_name}"å­—æ®µçš„å€¼ï¼š

        å‘ç¥¨æ–‡æœ¬ï¼š{text}

        è¦æ±‚ï¼š
        1. åªè¿”å›æå–çš„å€¼ï¼Œä¸è¦æœ‰ä»»ä½•è§£é‡Š
        2. å¦‚æœæ²¡æœ‰æ‰¾åˆ°ï¼Œè¿”å›"NOT_FOUND"
        3. ä¿æŒåŸå§‹æ ¼å¼

        {field_name}ï¼š
        """

        try:
            response = self.client.chat.completions.create(
                model="gpt-4o",
                messages=[{"role": "user", "content": prompt}],
                max_tokens=100,
                temperature=0.1
            )

            extracted_value = response.choices[0].message.content.strip()
            is_valid, confidence = self.validate_field_extraction(extracted_value, field_name)

            return (extracted_value, confidence) if is_valid else (None, 0.1)

        except Exception as e:
            print(f"LLMæå–å¤±è´¥ï¼š{e}")
            return None, 0.0

    def validate_field_extraction(self, value: str, field_name: str) -> Tuple[bool, float]:
        """éªŒè¯å­—æ®µæå–ç»“æœ"""
        if value == "NOT_FOUND" or value is None:
            return False, 0.0

        if field_name == "invoice_number":
            return self.validate_invoice_number_format(value)
        elif field_name.endswith("_date"):
            return self.validate_date(value)
        elif field_name == "amount":
            return self.validate_amount(value)
        elif field_name == "biller_name":
            return self.validate_company_name(value)

        return True, 0.8

    def validate_invoice_number_format(self, invoice_num: str) -> Tuple[bool, float]:
        """éªŒè¯å‘ç¥¨å·ç æ ¼å¼"""
        if len(invoice_num) < 4 or len(invoice_num) > 20:
            return False, 0.0

        if not re.match(r'^[A-Z0-9-]+$', invoice_num.upper()):
            return False, 0.2

        return True, 0.9

    def validate_date(self, date_str: str) -> Tuple[bool, float]:
        """éªŒè¯æ—¥æœŸæ ¼å¼"""
        try:
            datetime.strptime(date_str, '%Y-%m-%d')
            return True, 0.95
        except ValueError:
            return False, 0.0

    def validate_amount(self, amount) -> Tuple[bool, float]:
        """éªŒè¯é‡‘é¢"""
        try:
            if isinstance(amount, str):
                amount = float(amount.replace(',', ''))

            if amount <= 0 or amount > 1000000:  # åˆç†æ€§æ£€æŸ¥
                return False, 0.1

            return True, 0.95
        except (ValueError, TypeError):
            return False, 0.0

    def basic_extraction(self, text: str) -> Dict:
        """åŸºç¡€æå–æ–¹æ³•ï¼ˆä¸ä½¿ç”¨ç»„ä»¶è¯„ä¼°ï¼‰"""
        prompt = f"""
        è¯·ä»ä»¥ä¸‹å‘ç¥¨æ–‡æœ¬ä¸­æå–æ‰€æœ‰ä¿¡æ¯ï¼š

        å‘ç¥¨æ–‡æœ¬ï¼š{text}

        éœ€è¦æå–çš„å­—æ®µï¼š
        - invoice_number: å‘ç¥¨å·ç 
        - issue_date: å¼€ç¥¨æ—¥æœŸï¼ˆYYYY-MM-DDæ ¼å¼ï¼‰
        - amount: é‡‘é¢ï¼ˆæ•°å­—ï¼‰
        - biller_name: å¼€ç¥¨æ–¹åç§°

        è¯·ä»¥JSONæ ¼å¼è¿”å›æå–ç»“æœã€‚
        """

        try:
            response = self.client.chat.completions.create(
                model="gpt-4o",
                messages=[{"role": "user", "content": prompt}],
                response_format={"type": "json_object"}
            )

            result = json.loads(response.choices[0].message.content)
            result["confidence"] = 0.7  # é»˜è®¤ç½®ä¿¡åº¦
            return result
        except json.JSONDecodeError:
            return {"error": "JSONè§£æå¤±è´¥", "confidence": 0.0}

# ä½¿ç”¨ç¤ºä¾‹
def invoice_extraction_demo():
    """å‘ç¥¨æå–æ¼”ç¤º"""
    extractor = InvoiceExtractor()

    test_invoice = """
    åŒ—äº¬æ™ºèƒ½ç§‘æŠ€æœ‰é™å…¬å¸

    å‘ç¥¨å·ç ï¼šINV-2024-5678
    å¼€ç¥¨æ—¥æœŸï¼š2024å¹´6æœˆ15æ—¥
    é¡¹ç›®ï¼šAIç³»ç»Ÿå¼€å‘æœåŠ¡
    é‡‘é¢ï¼šï¿¥85,000.00
    """

    print("=== å‘ç¥¨ä¿¡æ¯æå–æ¼”ç¤º ===")
    result = extractor.extract_invoice_info(test_invoice, use_component_evaluation=True)

    print("æå–ç»“æœï¼š")
    for field, value in result.items():
        if field not in ["confidence", "field_confidences"]:
            print(f"{field}: {value}")

    print(f"\næ•´ä½“ç½®ä¿¡åº¦: {result.get('confidence', 0):.2f}")

if __name__ == "__main__":
    invoice_extraction_demo()
```

---

## 5. å®æˆ˜é¡¹ç›®2ï¼šå®¢æˆ·é‚®ä»¶å›å¤è´¨é‡æå‡

### 5.1 è´¨é‡è¯„ä¼°å™¨

```python
# quality_evaluator.py
from typing import Dict, List
import json
from openai import OpenAI
from config import OPENAI_API_KEY, MODEL, QUALITY_STANDARDS

client = OpenAI(api_key=OPENAI_API_KEY)

class QualityEvaluator:
    def __init__(self):
        self.client = client
        self.evaluation_history = []

    def evaluate_email_quality(self, email: str, context: Dict = None) -> Dict:
        """è¯„ä¼°é‚®ä»¶è´¨é‡"""
        print("ğŸ” è¯„ä¼°é‚®ä»¶è´¨é‡...")

        # å¤šç»´åº¦è¯„ä¼°
        dimension_scores = {}
        total_weight = 0
        weighted_sum = 0

        for dimension, config in QUALITY_STANDARDS.items():
            score = self.evaluate_dimension(email, dimension, config["description"], context)
            weight = config["weight"]

            dimension_scores[dimension] = {
                "score": score,
                "weight": weight,
                "max_possible": 5.0
            }

            weighted_sum += score * weight
            total_weight += weight

        # è®¡ç®—æ€»ä½“åˆ†æ•°
        overall_score = weighted_sum / total_weight if total_weight > 0 else 0

        # ç”Ÿæˆè¯¦ç»†åé¦ˆ
        detailed_feedback = self.generate_detailed_feedback(email, dimension_scores, context)

        result = {
            "overall_score": overall_score,
            "dimension_scores": dimension_scores,
            "detailed_feedback": detailed_feedback,
            "strengths": detailed_feedback.get("strengths", []),
            "improvement_areas": detailed_feedback.get("improvement_areas", []),
            "recommendations": detailed_feedback.get("recommendations", [])
        }

        # è®°å½•å†å²
        self.evaluation_history.append({
            "timestamp": datetime.now().isoformat(),
            "email": email[:200],  # ä¿å­˜å‰200å­—ç¬¦
            "evaluation": result
        })

        return result

    def evaluate_dimension(self, email: str, dimension: str, description: str, context: Dict = None) -> float:
        """è¯„ä¼°å•ä¸ªç»´åº¦"""
        context_str = json.dumps(context, ensure_ascii=False) if context else "æ— é¢å¤–ä¸Šä¸‹æ–‡"

        evaluation_prompt = f"""
        è¯·ä½œä¸ºä¸“ä¸šçš„å®¢æœè´¨é‡è¯„ä¼°å‘˜ï¼Œå¯¹ä»¥ä¸‹é‚®ä»¶çš„{dimension}è¿›è¡Œè¯„åˆ†ã€‚

        è¯„ä¼°ç»´åº¦ï¼š{dimension}
        è¯„ä¼°æ ‡å‡†ï¼š{description}

        é‚®ä»¶å†…å®¹ï¼š
        {email}

        é¢å¤–ä¸Šä¸‹æ–‡ï¼š
        {context_str}

        è¯·ç»™å‡º1-5åˆ†çš„è¯„åˆ†ï¼ˆ5åˆ†ä¸ºæœ€ä½³ï¼‰ï¼Œå¹¶æä¾›ç®€è¦çš„è¯„åˆ†ç†ç”±ã€‚

        è¿”å›æ ¼å¼ï¼š
        è¯„åˆ†ï¼š[1-5]
        ç†ç”±ï¼š[è¯„åˆ†ç†ç”±]
        """

        try:
            response = self.client.chat.completions.create(
                model=MODEL,
                messages=[{"role": "user", "content": evaluation_prompt}]
            )

            result = response.choices[0].message.content.strip()

            # è§£æè¯„åˆ†
            lines = result.split('\n')
            score_line = next((line for line in lines if 'è¯„åˆ†ï¼š' in line), '')

            # æå–æ•°å­—è¯„åˆ†
            import re
            score_match = re.search(r'(\d+)', score_line)
            if score_match:
                score = float(score_match.group(1))
                return max(1, min(5, score))  # ç¡®ä¿åœ¨1-5èŒƒå›´å†…

            return 3.0  # é»˜è®¤åˆ†æ•°

        except Exception as e:
            print(f"ç»´åº¦è¯„ä¼°å¤±è´¥ï¼š{e}")
            return 3.0

    def generate_detailed_feedback(self, email: str, dimension_scores: Dict, context: Dict = None) -> Dict:
        """ç”Ÿæˆè¯¦ç»†åé¦ˆ"""
        feedback_prompt = f"""
        åŸºäºä»¥ä¸‹é‚®ä»¶çš„è¯„ä¼°ç»“æœï¼Œè¯·æä¾›è¯¦ç»†çš„åé¦ˆå’Œæ”¹è¿›å»ºè®®ï¼š

        é‚®ä»¶å†…å®¹ï¼š
        {email}

        å„ç»´åº¦è¯„åˆ†ï¼š
        {json.dumps(dimension_scores, ensure_ascii=False, indent=2)}

        è¯·æä¾›ï¼š
        1. é‚®ä»¶çš„ä¸»è¦ä¼˜ç‚¹ï¼ˆstrengthsï¼‰
        2. éœ€è¦æ”¹è¿›çš„æ–¹é¢ï¼ˆimprovement_areasï¼‰
        3. å…·ä½“çš„æ”¹è¿›å»ºè®®ï¼ˆrecommendationsï¼‰
        4. æ€»ä½“è¯„ä»·ï¼ˆoverall_assessmentï¼‰

        è¿”å›JSONæ ¼å¼ã€‚
        """

        try:
            response = self.client.chat.completions.create(
                model=MODEL,
                messages=[{"role": "user", "content": feedback_prompt}],
                response_format={"type": "json_object"}
            )

            feedback = json.loads(response.choices[0].message.content)
            return feedback

        except Exception as e:
            print(f"è¯¦ç»†åé¦ˆç”Ÿæˆå¤±è´¥ï¼š{e}")
            return {
                "strengths": ["é‚®ä»¶ç»“æ„å®Œæ•´"],
                "improvement_areas": ["å¯ä»¥æ›´åŠ å…·ä½“"],
                "recommendations": ["æ·»åŠ æ›´å¤šç»†èŠ‚"],
                "overall_assessment": "è´¨é‡è‰¯å¥½ï¼Œæœ‰æ”¹è¿›ç©ºé—´"
            }

    def compare_multiple_variants(self, variants: List[Dict], context: Dict = None) -> Dict:
        """æ¯”è¾ƒå¤šä¸ªé‚®ä»¶å˜ä½“"""
        print("ğŸ” æ¯”è¾ƒå¤šä¸ªé‚®ä»¶å˜ä½“...")

        # è¯„ä¼°æ¯ä¸ªå˜ä½“
        for variant in variants:
            evaluation = self.evaluate_email_quality(variant["email"], context)
            variant["quality_score"] = evaluation["overall_score"]
            variant["detailed_evaluation"] = evaluation

        # æ’åºå˜ä½“
        sorted_variants = sorted(variants, key=lambda x: x["quality_score"], reverse=True)

        comparison_result = {
            "variants_evaluated": len(variants),
            "best_variant": sorted_variants[0] if sorted_variants else None,
            "ranking": [
                {
                    "rank": i + 1,
                    "variant_id": variant["variant_id"],
                    "quality_score": variant["quality_score"],
                    "key_strengths": variant["detailed_evaluation"]["strengths"][:3]
                }
                for i, variant in enumerate(sorted_variants)
            ],
            "score_distribution": self.analyze_score_distribution(variants),
            "common_strengths": self.identify_common_strengths(variants),
            "common_weaknesses": self.identify_common_weaknesses(variants)
        }

        return comparison_result

    def analyze_score_distribution(self, variants: List[Dict]) -> Dict:
        """åˆ†æåˆ†æ•°åˆ†å¸ƒ"""
        scores = [variant["quality_score"] for variant in variants]

        if not scores:
            return {}

        return {
            "min_score": min(scores),
            "max_score": max(scores),
            "average_score": sum(scores) / len(scores),
            "score_range": max(scores) - min(scores),
            "standard_deviation": self.calculate_std_dev(scores)
        }

    def calculate_std_dev(self, scores: List[float]) -> float:
        """è®¡ç®—æ ‡å‡†å·®"""
        if len(scores) <= 1:
            return 0.0

        mean = sum(scores) / len(scores)
        variance = sum((score - mean) ** 2 for score in scores) / len(scores)
        return variance ** 0.5

    def identify_common_strengths(self, variants: List[Dict]) -> List[str]:
        """è¯†åˆ«å…±åŒä¼˜ç‚¹"""
        all_strengths = []
        for variant in variants:
            strengths = variant["detailed_evaluation"].get("strengths", [])
            all_strengths.extend(strengths)

        # ç»Ÿè®¡å‡ºç°é¢‘ç‡
        strength_counts = {}
        for strength in all_strengths:
            strength_counts[strength] = strength_counts.get(strength, 0) + 1

        # è¿”å›å‡ºç°é¢‘ç‡æœ€é«˜çš„ä¼˜ç‚¹
        sorted_strengths = sorted(strength_counts.items(), key=lambda x: x[1], reverse=True)
        return [strength for strength, count in sorted_strengths[:5]]

    def identify_common_weaknesses(self, variants: List[Dict]) -> List[str]:
        """è¯†åˆ«å…±åŒå¼±ç‚¹"""
        all_weaknesses = []
        for variant in variants:
            weaknesses = variant["detailed_evaluation"].get("improvement_areas", [])
            all_weaknesses.extend(weaknesses)

        # ç»Ÿè®¡å‡ºç°é¢‘ç‡
        weakness_counts = {}
        for weakness in all_weaknesses:
            weakness_counts[weakness] = weakness_counts.get(weakness, 0) + 1

        # è¿”å›å‡ºç°é¢‘ç‡æœ€é«˜çš„å¼±ç‚¹
        sorted_weaknesses = sorted(weakness_counts.items(), key=lambda x: x[1], reverse=True)
        return [weakness for weakness, count in sorted_weaknesses[:5]]

# ä½¿ç”¨ç¤ºä¾‹
def quality_evaluation_demo():
    """è´¨é‡è¯„ä¼°æ¼”ç¤º"""
    evaluator = QualityEvaluator()

    # æµ‹è¯•é‚®ä»¶
    test_email = """
    å°Šæ•¬çš„å¼ å…ˆç”Ÿï¼Œ

    æ„Ÿè°¢æ‚¨è”ç³»æˆ‘ä»¬çš„å®¢æœå›¢é˜Ÿã€‚

    å…³äºæ‚¨åæ˜ çš„æ™ºèƒ½æ‰‹è¡¨ç”µæ± ç»­èˆªé—®é¢˜ï¼Œæˆ‘ä»¬æ·±è¡¨æ­‰æ„ã€‚ç»è¿‡æŠ€æœ¯å›¢é˜Ÿåˆ†æï¼Œå¯èƒ½æ˜¯ä»¥ä¸‹åŸå› å¯¼è‡´ï¼š

    1. é¦–æ¬¡ä½¿ç”¨æ—¶çš„ç³»ç»Ÿæ›´æ–°æ¶ˆè€—è¾ƒå¤šç”µé‡
    2. æŸäº›åå°åº”ç”¨å¯èƒ½æœªä¼˜åŒ–
    3. æç«¯æ¸©åº¦ç¯å¢ƒå½±å“ç”µæ± æ€§èƒ½

    å»ºè®®æ‚¨å°è¯•ä»¥ä¸‹è§£å†³æ–¹æ¡ˆï¼š
    - å®Œæˆæ‰€æœ‰ç³»ç»Ÿæ›´æ–°
    - å…³é—­ä¸å¿…è¦çš„åå°åº”ç”¨
    - åœ¨å¸¸æ¸©ç¯å¢ƒä¸‹ä½¿ç”¨

    å¦‚æœé—®é¢˜ä»ç„¶å­˜åœ¨ï¼Œæˆ‘ä»¬å¯ä»¥ä¸ºæ‚¨å®‰æ’å…è´¹æ£€æµ‹æˆ–æ›´æ¢ã€‚æ‚¨å¸Œæœ›é€‰æ‹©å“ªç§æ–¹å¼ï¼Ÿ

    å†æ¬¡ä¸ºç»™æ‚¨å¸¦æ¥çš„ä¸ä¾¿é“æ­‰ã€‚

    æ­¤è‡´
    æ•¬ç¤¼

    å®¢æœå›¢é˜Ÿ
    """

    print("=== é‚®ä»¶è´¨é‡è¯„ä¼°æ¼”ç¤º ===")
    result = evaluator.evaluate_email_quality(test_email)

    print(f"æ€»ä½“è¯„åˆ†: {result['overall_score']:.2f}/5.0")
    print(f"\nå„ç»´åº¦è¯„åˆ†:")
    for dimension, scores in result['dimension_scores'].items():
        print(f"  {dimension}: {scores['score']:.1f}/5.0 (æƒé‡: {scores['weight']})")

    print(f"\nä¸»è¦ä¼˜ç‚¹:")
    for strength in result['strengths']:
        print(f"  - {strength}")

    print(f"\næ”¹è¿›å»ºè®®:")
    for recommendation in result['recommendations']:
        print(f"  - {recommendation}")

if __name__ == "__main__":
    quality_evaluation_demo()
```

---

## 6. å»¶è¿Ÿä¸æˆæœ¬ä¼˜åŒ–

### 6.1 æ€§èƒ½ç›‘æ§å™¨

```python
# performance_monitor.py
import time
import psutil
from typing import Dict, List, Callable
from dataclasses import dataclass
from datetime import datetime

@dataclass
class PerformanceMetrics:
    execution_time: float
    memory_usage: float
    cpu_usage: float
    api_calls: int
    cost_estimate: float
    timestamp: datetime

class PerformanceMonitor:
    def __init__(self):
        self.metrics_history = []
        self.api_call_count = 0
        self.cost_per_1k_tokens = 0.03  # GPT-4oä»·æ ¼

    def monitor_execution(self, func: Callable, *args, **kwargs) -> tuple:
        """ç›‘æ§å‡½æ•°æ‰§è¡Œæ€§èƒ½"""
        # å¼€å§‹ç›‘æ§
        start_time = time.time()
        start_memory = psutil.Process().memory_info().rss / 1024 / 1024  # MB
        start_cpu = psutil.cpu_percent()

        # é‡ç½®APIè®¡æ•°
        self.api_call_count = 0

        # æ‰§è¡Œå‡½æ•°
        result = func(*args, **kwargs)

        # ç»“æŸç›‘æ§
        end_time = time.time()
        end_memory = psutil.Process().memory_info().rss / 1024 / 1024  # MB
        end_cpu = psutil.cpu_percent()

        # è®¡ç®—æŒ‡æ ‡
        execution_time = end_time - start_time
        memory_usage = end_memory - start_memory
        cpu_usage = end_cpu - start_cpu

        # ä¼°ç®—æˆæœ¬
        cost_estimate = self.estimate_cost(self.api_call_count)

        metrics = PerformanceMetrics(
            execution_time=execution_time,
            memory_usage=max(0, memory_usage),
            cpu_usage=cpu_usage,
            api_calls=self.api_call_count,
            cost_estimate=cost_estimate,
            timestamp=datetime.now()
        )

        self.metrics_history.append(metrics)

        return result, metrics

    def estimate_cost(self, api_calls: int) -> float:
        """ä¼°ç®—APIè°ƒç”¨æˆæœ¬"""
        # ç®€åŒ–ä¼°ç®—ï¼šå‡è®¾æ¯æ¬¡è°ƒç”¨å¹³å‡500 tokens
        avg_tokens_per_call = 500
        total_tokens = api_calls * avg_tokens_per_call

        return (total_tokens / 1000) * self.cost_per_1k_tokens

    def get_performance_summary(self) -> Dict:
        """è·å–æ€§èƒ½æ€»ç»“"""
        if not self.metrics_history:
            return {}

        recent_metrics = self.metrics_history[-10:]  # æœ€è¿‘10æ¬¡

        return {
            "average_execution_time": sum(m.execution_time for m in recent_metrics) / len(recent_metrics),
            "average_memory_usage": sum(m.memory_usage for m in recent_metrics) / len(recent_metrics),
            "average_cost": sum(m.cost_estimate for m in recent_metrics) / len(recent_metrics),
            "total_api_calls": sum(m.api_calls for m in recent_metrics),
            "performance_trend": self.analyze_trend()
        }

    def analyze_trend(self) -> str:
        """åˆ†ææ€§èƒ½è¶‹åŠ¿"""
        if len(self.metrics_history) < 5:
            return "æ•°æ®ä¸è¶³"

        recent = self.metrics_history[-5:]
        older = self.metrics_history[-10:-5] if len(self.metrics_history) >= 10 else self.metrics_history[:5]

        recent_avg_time = sum(m.execution_time for m in recent) / len(recent)
        older_avg_time = sum(m.execution_time for m in older) / len(older)

        if recent_avg_time < older_avg_time * 0.9:
            return "æ€§èƒ½æå‡"
        elif recent_avg_time > older_avg_time * 1.1:
            return "æ€§èƒ½ä¸‹é™"
        else:
            return "æ€§èƒ½ç¨³å®š"

    def identify_bottlenecks(self) -> List[Dict]:
        """è¯†åˆ«æ€§èƒ½ç“¶é¢ˆ"""
        bottlenecks = []

        if not self.metrics_history:
            return bottlenecks

        # åˆ†ææ‰§è¡Œæ—¶é—´
        avg_time = sum(m.execution_time for m in self.metrics_history) / len(self.metrics_history)

        for i, metrics in enumerate(self.metrics_history):
            if metrics.execution_time > avg_time * 1.5:
                bottlenecks.append({
                    "type": "slow_execution",
                    "timestamp": metrics.timestamp,
                    "execution_time": metrics.execution_time,
                    "api_calls": metrics.api_calls,
                    "suggestion": "æ£€æŸ¥APIè°ƒç”¨æ•°é‡å’Œå¤æ‚åº¦"
                })

            if metrics.cost_estimate > 0.1:  # æˆæœ¬è¶…è¿‡10ç¾åˆ†
                bottlenecks.append({
                    "type": "high_cost",
                    "timestamp": metrics.timestamp,
                    "cost": metrics.cost_estimate,
                    "api_calls": metrics.api_calls,
                    "suggestion": "ä¼˜åŒ–APIä½¿ç”¨æˆ–å‡å°‘è°ƒç”¨æ¬¡æ•°"
                })

        return bottlenecks

# æˆæœ¬ä¼˜åŒ–ç­–ç•¥
class CostOptimizer:
    def __init__(self):
        self.optimization_strategies = {
            "model_downgrade": {
                "description": "é™çº§åˆ°æ›´ä¾¿å®œçš„æ¨¡å‹",
                "cost_reduction": 0.7,
                "quality_impact": 0.1
            },
            "caching": {
                "description": "ç¼“å­˜é‡å¤æŸ¥è¯¢",
                "cost_reduction": 0.5,
                "quality_impact": 0.0
            },
            "batch_processing": {
                "description": "æ‰¹é‡å¤„ç†è¯·æ±‚",
                "cost_reduction": 0.3,
                "quality_impact": 0.02
            },
            "prompt_optimization": {
                "description": "ä¼˜åŒ–æç¤ºè¯é•¿åº¦",
                "cost_reduction": 0.2,
                "quality_impact": 0.01
            }
        }

    def generate_cost_optimization_plan(self, current_costs: Dict) -> Dict:
        """ç”Ÿæˆæˆæœ¬ä¼˜åŒ–è®¡åˆ’"""
        monthly_cost = current_costs.get("monthly_api_cost", 0)
        target_reduction = current_costs.get("target_reduction", 0.3)

        applicable_strategies = []

        for strategy_name, strategy_info in self.optimization_strategies.items():
            potential_saving = monthly_cost * strategy_info["cost_reduction"]

            applicable_strategies.append({
                "strategy": strategy_name,
                "description": strategy_info["description"],
                "cost_reduction": strategy_info["cost_reduction"],
                "quality_impact": strategy_info["quality_impact"],
                "potential_saving": potential_saving,
                "priority": "high" if strategy_info["cost_reduction"] > 0.4 else "medium"
            })

        # æŒ‰èŠ‚çœæ½œåŠ›æ’åº
        applicable_strategies.sort(key=lambda x: x["potential_saving"], reverse=True)

        return {
            "current_monthly_cost": monthly_cost,
            "target_reduction": target_reduction,
            "recommended_strategies": applicable_strategies[:3],  # å‰3ä¸ªæœ€ä½³ç­–ç•¥
            "expected_savings": sum(s["potential_saving"] for s in applicable_strategies[:3]),
            "implementation_timeline": "2-4å‘¨"
        }

# å»¶è¿Ÿä¼˜åŒ–ç­–ç•¥
class LatencyOptimizer:
    def __init__(self):
        self.latency_strategies = {
            "parallel_execution": {
                "description": "å¹¶è¡Œæ‰§è¡Œç‹¬ç«‹ä»»åŠ¡",
                "latency_reduction": 0.4,
                "complexity": "medium"
            },
            "caching": {
                "description": "ç¼“å­˜ä¸­é—´ç»“æœ",
                "latency_reduction": 0.3,
                "complexity": "low"
            },
            "streaming": {
                "description": "æµå¼å¤„ç†",
                "latency_reduction": 0.5,
                "complexity": "high"
            },
            "model_selection": {
                "description": "é€‰æ‹©æ›´å¿«çš„æ¨¡å‹",
                "latency_reduction": 0.6,
                "complexity": "low"
            }
        }

    def analyze_latency_bottlenecks(self, performance_data: List[Dict]) -> Dict:
        """åˆ†æå»¶è¿Ÿç“¶é¢ˆ"""
        if not performance_data:
            return {}

        # åˆ†æå„æ­¥éª¤è€—æ—¶
        step_times = {}
        for data in performance_data:
            if "step_breakdown" in data:
                for step, time_taken in data["step_breakdown"].items():
                    if step not in step_times:
                        step_times[step] = []
                    step_times[step].append(time_taken)

        # æ‰¾å‡ºæœ€è€—æ—¶çš„æ­¥éª¤
        bottleneck_analysis = []
        for step, times in step_times.items():
            avg_time = sum(times) / len(times)
            max_time = max(times)

            bottleneck_analysis.append({
                "step": step,
                "average_time": avg_time,
                "max_time": max_time,
                "bottleneck_severity": "high" if avg_time > 1.0 else "medium" if avg_time > 0.5 else "low"
            })

        # æŒ‰ä¸¥é‡ç¨‹åº¦æ’åº
        bottleneck_analysis.sort(key=lambda x: x["average_time"], reverse=True)

        return {
            "bottlenecks": bottleneck_analysis[:3],  # æœ€ä¸¥é‡çš„3ä¸ªç“¶é¢ˆ
            "total_steps": len(step_times),
            "recommendations": self.generate_latency_recommendations(bottleneck_analysis)
        }

    def generate_latency_recommendations(self, bottlenecks: List[Dict]) -> List[Dict]:
        """ç”Ÿæˆå»¶è¿Ÿä¼˜åŒ–å»ºè®®"""
        recommendations = []

        for bottleneck in bottlenecks:
            step = bottleneck["step"]
            severity = bottleneck["bottleneck_severity"]

            if "llm" in step.lower() or "api" in step.lower():
                recommendations.append({
                    "bottleneck": step,
                    "suggestion": "è€ƒè™‘ä½¿ç”¨æ›´å¿«çš„æ¨¡å‹æˆ–æ·»åŠ ç¼“å­˜",
                    "strategy": "model_selection",
                    "priority": severity
                })
            elif "processing" in step.lower():
                recommendations.append({
                    "bottleneck": step,
                    "suggestion": "è€ƒè™‘å¹¶è¡Œå¤„ç†æˆ–ä¼˜åŒ–ç®—æ³•",
                    "strategy": "parallel_execution",
                    "priority": severity
                })
            else:
                recommendations.append({
                    "bottleneck": step,
                    "suggestion": "åˆ†ææ˜¯å¦å¯ä»¥ä¼˜åŒ–æˆ–å¹¶è¡ŒåŒ–",
                    "strategy": "general_optimization",
                    "priority": severity
                })

        return recommendations

# ä½¿ç”¨ç¤ºä¾‹
def performance_optimization_demo():
    """æ€§èƒ½ä¼˜åŒ–æ¼”ç¤º"""
    monitor = PerformanceMonitor()
    cost_optimizer = CostOptimizer()
    latency_optimizer = LatencyOptimizer()

    # æ¨¡æ‹Ÿæ€§èƒ½æ•°æ®
    current_costs = {
        "monthly_api_cost": 150.0,  # $150/æœˆ
        "target_reduction": 0.3     # å¸Œæœ›å‡å°‘30%
    }

    print("=== æ€§èƒ½ä¼˜åŒ–æ¼”ç¤º ===")

    # æˆæœ¬ä¼˜åŒ–
    cost_plan = cost_optimizer.generate_cost_optimization_plan(current_costs)
    print(f"å½“å‰æœˆåº¦æˆæœ¬: ${cost_plan['current_monthly_cost']}")
    print(f"é¢„æœŸèŠ‚çœ: ${cost_plan['expected_savings']:.2f}")

    # å»¶è¿Ÿä¼˜åŒ–
    performance_data = [
        {"step_breakdown": {"llm_processing": 2.5, "data_processing": 0.8}},
        {"step_breakdown": {"llm_processing": 3.1, "data_processing": 0.9}}
    ]

    latency_analysis = latency_optimizer.analyze_latency_bottlenecks(performance_data)
    print(f"\nå‘ç°çš„ç“¶é¢ˆ: {len(latency_analysis['bottlenecks'])}")
    for bottleneck in latency_analysis['bottlenecks']:
        print(f"  - {bottleneck['step']}: {bottleneck['average_time']:.2f}s ({bottleneck['bottleneck_severity']})")

if __name__ == "__main__":
    performance_optimization_demo()
```

---

## 7. å¼€å‘æµç¨‹æœ€ä½³å®è·µ

### 7.1 å¼€å‘è¿­ä»£å››é˜¶æ®µ

| é˜¶æ®µ | æè¿° | ä¸»è¦æ´»åŠ¨ | å…³é”®æŒ‡æ ‡ |
|------|------|----------|----------|
| **1. å¿«é€ŸåŸå‹** | å¿«é€Ÿæ„å»ºç«¯åˆ°ç«¯ç³»ç»Ÿ | æ‰‹åŠ¨æ£€æŸ¥è¾“å‡ºï¼Œå‡­ç›´è§‰æ‰¾é—®é¢˜ | åŠŸèƒ½å®Œæ•´æ€§ |
| **2. åˆæ­¥è¯„ä¼°** | ç³»ç»Ÿå¼€å§‹æˆç†Ÿ | æ„å»ºå°å‹ç«¯åˆ°ç«¯è¯„ä¼°ï¼ˆ10-20ä¾‹ï¼‰ | åŸºç¡€å‡†ç¡®ç‡ |
| **3. ä¸¥è°¨åˆ†æ** | éœ€è¦ç²¾ç¡®æ”¹è¿›æ–¹å‘ | è¿›è¡Œé”™è¯¯åˆ†æï¼Œç»Ÿè®¡é‡åŒ–é—®é¢˜ | é”™è¯¯åˆ†å¸ƒ |
| **4. é«˜æ•ˆè°ƒä¼˜** | ç»„ä»¶çº§é«˜æ•ˆæ”¹è¿› | æ„å»ºç»„ä»¶çº§è¯„ä¼°è¿›è¡Œè°ƒä¼˜ | ç»„ä»¶å‡†ç¡®ç‡ |

### 7.2 å®Œæ•´å¼€å‘å·¥ä½œæµ

åˆ›å»ºæ–‡ä»¶ `development_workflow.py`ï¼š

```python
# development_workflow.py
from typing import Dict, List
from dataclasses import dataclass
from datetime import datetime

@dataclass
class DevelopmentStage:
    name: str
    description: str
    key_activities: List[str]
    success_criteria: List[str]
    common_pitfalls: List[str]
    recommended_tools: List[str]
    estimated_time: str

class DevelopmentWorkflow:
    def __init__(self):
        self.stages = self.define_development_stages()
        self.current_stage = 0
        self.stage_history = []

    def define_development_stages(self) -> List[DevelopmentStage]:
        """å®šä¹‰å¼€å‘é˜¶æ®µ"""
        return [
            DevelopmentStage(
                name="å¿«é€ŸåŸå‹",
                description="å¿«é€Ÿæ„å»ºå¯å·¥ä½œçš„ç«¯åˆ°ç«¯ç³»ç»Ÿ",
                key_activities=[
                    "æ­å»ºåŸºç¡€å·¥ä½œæµæ¶æ„",
                    "å®ç°æ ¸å¿ƒåŠŸèƒ½",
                    "æ‰‹åŠ¨æµ‹è¯•åŸºæœ¬æµç¨‹",
                    "è¯†åˆ«æ˜æ˜¾é—®é¢˜"
                ],
                success_criteria=[
                    "ç³»ç»Ÿèƒ½å¤Ÿå®ŒæˆåŸºæœ¬ä»»åŠ¡",
                    "ä¸»è¦æµç¨‹å¯ä»¥è¿è¡Œ",
                    "æ²¡æœ‰é˜»å¡æ€§bug"
                ],
                common_pitfalls=[
                    "è¿‡åº¦è®¾è®¡",
                    "è¿‡æ—©ä¼˜åŒ–",
                    "è¿½æ±‚å®Œç¾"
                ],
                recommended_tools=[
                    "ç®€å•LLMè°ƒç”¨",
                    "åŸºç¡€å·¥å…·å‡½æ•°",
                    "æ‰‹åŠ¨æµ‹è¯•"
                ],
                estimated_time="1-2å‘¨"
            ),
            DevelopmentStage(
                name="åˆæ­¥è¯„ä¼°",
                description="å»ºç«‹åŸºç¡€è¯„ä¼°ä½“ç³»",
                key_activities=[
                    "æ„å»º10-20ä¸ªæµ‹è¯•æ¡ˆä¾‹",
                    "å®ç°åŸºç¡€è¯„ä¼°æŒ‡æ ‡",
                    "è¿è¡Œåˆæ­¥è¯„ä¼°",
                    "è¯†åˆ«ä¸»è¦é—®é¢˜"
                ],
                success_criteria=[
                    "æœ‰æ˜ç¡®çš„å‡†ç¡®ç‡æ•°å­—",
                    "çŸ¥é“ä¸»è¦é—®é¢˜åœ¨å“ªé‡Œ",
                    "æœ‰æ”¹è¿›æ–¹å‘"
                ],
                common_pitfalls=[
                    "è¯„ä¼°æ¡ˆä¾‹å¤ªå°‘",
                    "è¯„ä¼°æŒ‡æ ‡ä¸åˆç†",
                    "å¿½è§†è¾¹ç¼˜æƒ…å†µ"
                ],
                recommended_tools=[
                    "è¯„ä¼°æ¡†æ¶",
                    "æµ‹è¯•æ•°æ®é›†",
                    "æŒ‡æ ‡è®¡ç®—å·¥å…·"
                ],
                estimated_time="1å‘¨"
            ),
            DevelopmentStage(
                name="é”™è¯¯åˆ†æ",
                description="æ·±å…¥åˆ†æé”™è¯¯æ¨¡å¼",
                key_activities=[
                    "æ”¶é›†æ‰§è¡Œè¿½è¸ª",
                    "ç»Ÿè®¡é”™è¯¯åˆ†å¸ƒ",
                    "è¯†åˆ«é”™è¯¯æ¨¡å¼",
                    "ç¡®å®šä¼˜å…ˆçº§"
                ],
                success_criteria=[
                    "çŸ¥é“æ¯ç§é”™è¯¯çš„æ¯”ä¾‹",
                    "è¯†åˆ«å‡ºä¸»è¦é”™è¯¯æ¨¡å¼",
                    "æœ‰æ˜ç¡®çš„ä¼˜å…ˆçº§æ’åº"
                ],
                common_pitfalls=[
                    "å‡­æ„Ÿè§‰åˆ¤æ–­",
                    "å¿½è§†æ•°æ®ç»Ÿè®¡",
                    "è¯•å›¾è§£å†³æ‰€æœ‰é—®é¢˜"
                ],
                recommended_tools=[
                    "é”™è¯¯åˆ†æå·¥å…·",
                    "ç»Ÿè®¡åˆ†æ",
                    "å¯è§†åŒ–å·¥å…·"
                ],
                estimated_time="3-5å¤©"
            ),
            DevelopmentStage(
                name="é«˜æ•ˆè°ƒä¼˜",
                description="é’ˆå¯¹æ€§ç»„ä»¶çº§ä¼˜åŒ–",
                key_activities=[
                    "æ„å»ºç»„ä»¶çº§è¯„ä¼°",
                    "é’ˆå¯¹æ€§æ”¹è¿›",
                    "éªŒè¯æ”¹è¿›æ•ˆæœ",
                    "è¿­ä»£ä¼˜åŒ–"
                ],
                success_criteria=[
                    "ç»„ä»¶å‡†ç¡®ç‡è¾¾åˆ°ç›®æ ‡",
                    "æ•´ä½“æ€§èƒ½æå‡",
                    "æˆæœ¬æ§åˆ¶åœ¨é¢„ç®—å†…"
                ],
                common_pitfalls=[
                    "è¿‡åº¦ä¼˜åŒ–",
                    "å¿½è§†æ•´ä½“å½±å“",
                    "ç¼ºä¹éªŒè¯"
                ],
                recommended_tools=[
                    "ç»„ä»¶è¯„ä¼°å™¨",
                    "A/Bæµ‹è¯•æ¡†æ¶",
                    "æ€§èƒ½ç›‘æ§å·¥å…·"
                ],
                estimated_time="2-4å‘¨"
            )
        ]

    def get_current_stage_guidance(self) -> DevelopmentStage:
        """è·å–å½“å‰é˜¶æ®µçš„æŒ‡å¯¼"""
        if 0 <= self.current_stage < len(self.stages):
            return self.stages[self.current_stage]
        else:
            return None

    def move_to_next_stage(self) -> bool:
        """è¿›å…¥ä¸‹ä¸€é˜¶æ®µ"""
        if self.current_stage < len(self.stages) - 1:
            self.stage_history.append({
                "stage": self.stages[self.current_stage],
                "completed_at": datetime.now(),
                "status": "completed"
            })
            self.current_stage += 1
            return True
        return False

    def generate_stage_checklist(self, stage: DevelopmentStage) -> List[Dict]:
        """ç”Ÿæˆé˜¶æ®µæ£€æŸ¥æ¸…å•"""
        checklist = []

        for i, activity in enumerate(stage.key_activities, 1):
            checklist.append({
                "item_id": i,
                "activity": activity,
                "completed": False,
                "notes": "",
                "evidence": None
            })

        for i, criterion in enumerate(stage.success_criteria, len(stage.key_activities) + 1):
            checklist.append({
                "item_id": i,
                "criterion": criterion,
                "met": False,
                "evidence": None
            })

        return checklist

    def assess_stage_completion(self, stage: DevelopmentStage, checklist: List[Dict]) -> Dict:
        """è¯„ä¼°é˜¶æ®µå®Œæˆæƒ…å†µ"""
        activities_completed = sum(1 for item in checklist if item.get("completed", False))
        criteria_met = sum(1 for item in checklist if item.get("met", False))

        total_activities = len(stage.key_activities)
        total_criteria = len(stage.success_criteria)

        completion_percentage = (activities_completed + criteria_met) / (total_activities + total_criteria)

        assessment = {
            "stage_name": stage.name,
            "completion_percentage": completion_percentage,
            "activities_completed": activities_completed,
            "total_activities": total_activities,
            "criteria_met": criteria_met,
            "total_criteria": total_criteria,
            "status": self.determine_stage_status(completion_percentage),
            "recommendations": self.generate_completion_recommendations(stage, checklist),
            "ready_for_next_stage": completion_percentage >= 0.8
        }

        return assessment

    def determine_stage_status(self, completion_percentage: float) -> str:
        """ç¡®å®šé˜¶æ®µçŠ¶æ€"""
        if completion_percentage >= 0.9:
            return "excellent"
        elif completion_percentage >= 0.8:
            return "good"
        elif completion_percentage >= 0.6:
            return "fair"
        else:
            return "needs_improvement"

    def generate_completion_recommendations(self, stage: DevelopmentStage, checklist: List[Dict]) -> List[str]:
        """ç”Ÿæˆå®Œæˆå»ºè®®"""
        recommendations = []

        # æ£€æŸ¥æœªå®Œæˆçš„æ´»åŠ¨
        incomplete_activities = [item for item in checklist if item.get("activity") and not item.get("completed", False)]
        if incomplete_activities:
            recommendations.append(f"å®Œæˆå‰©ä½™ {len(incomplete_activities)} ä¸ªå…³é”®æ´»åŠ¨")

        # æ£€æŸ¥æœªæ»¡è¶³çš„æ ‡å‡†
        unmet_criteria = [item for item in checklist if item.get("criterion") and not item.get("met", False)]
        if unmet_criteria:
            recommendations.append(f"æ»¡è¶³å‰©ä½™ {len(unmet_criteria)} ä¸ªæˆåŠŸæ ‡å‡†")

        # åŸºäºå¸¸è§é™·é˜±çš„å»ºè®®
        recommendations.extend(self.generate_pitfall_avoidance_tips(stage))

        return recommendations

    def generate_pitfall_avoidance_tips(self, stage: DevelopmentStage) -> List[str]:
        """ç”Ÿæˆé¿å…é™·é˜±çš„å»ºè®®"""
        tips = []

        for pitfall in stage.common_pitfalls:
            if "è¿‡åº¦" in pitfall:
                tips.append("ä¿æŒç®€å•ï¼Œå…ˆè®©åŸºæœ¬åŠŸèƒ½å·¥ä½œ")
            elif "è¿‡æ—©" in pitfall:
                tips.append("å…ˆå®Œæˆå½“å‰é˜¶æ®µï¼Œå†è€ƒè™‘ä¸‹ä¸€é˜¶æ®µ")
            elif "å¿½è§†" in pitfall:
                tips.append(f"ç‰¹åˆ«æ³¨æ„ï¼š{pitfall}")
            else:
                tips.append(f"é¿å…ï¼š{pitfall}")

        return tips

# æœ€ä½³å®è·µæ£€æŸ¥æ¸…å•
class BestPracticeChecker:
    def __init__(self):
        self.best_practices = {
            "evaluation": [
                "æ¯ä¸ªç»„ä»¶éƒ½æœ‰è¯„ä¼°æ–¹æ³•",
                "è¯„ä¼°æŒ‡æ ‡ä¸ä¸šåŠ¡ç›®æ ‡ä¸€è‡´",
                "å®šæœŸé‡æ–°è¯„ä¼°",
                "ä¿å­˜è¯„ä¼°å†å²è®°å½•"
            ],
            "error_handling": [
                "æœ‰å®Œå–„çš„é”™è¯¯å¤„ç†æœºåˆ¶",
                "é”™è¯¯ä¿¡æ¯è¯¦ç»†ä¸”æœ‰ç”¨",
                "è®°å½•é”™è¯¯å‘ç”Ÿä¸Šä¸‹æ–‡",
                "æœ‰é”™è¯¯æ¢å¤ç­–ç•¥"
            ],
            "optimization": [
                "åŸºäºæ•°æ®åšä¼˜åŒ–å†³ç­–",
                "ä¸€æ¬¡åªæ”¹å˜ä¸€ä¸ªå˜é‡",
                "æœ‰æ˜ç¡®çš„ä¼˜åŒ–ç›®æ ‡",
                "éªŒè¯ä¼˜åŒ–æ•ˆæœ"
            ],
            "documentation": [
                "ä»£ç æœ‰æ¸…æ™°æ³¨é‡Š",
                "æœ‰ç³»ç»Ÿæ¶æ„æ–‡æ¡£",
                "æœ‰ä½¿ç”¨è¯´æ˜",
                "è®°å½•è®¾è®¡å†³ç­–"
            ]
        }

    def generate_checklist(self, category: str = None) -> Dict:
        """ç”Ÿæˆæœ€ä½³å®è·µæ£€æŸ¥æ¸…å•"""
        if category and category in self.best_practices:
            practices = {category: self.best_practices[category]}
        else:
            practices = self.best_practices

        checklist = {}
        for cat, practices_list in practices.items():
            checklist[cat] = [
                {
                    "practice": practice,
                    "implemented": False,
                    "evidence": None,
                    "priority": "high" if i < 2 else "medium"
                }
                for i, practice in enumerate(practices_list)
            ]

        return checklist

    def assess_best_practices(self, checklist: Dict) -> Dict:
        """è¯„ä¼°æœ€ä½³å®è·µéµå¾ªæƒ…å†µ"""
        assessment = {}

        for category, practices in checklist.items():
            total_practices = len(practices)
            implemented_practices = sum(1 for p in practices if p["implemented"])

            high_priority_total = sum(1 for p in practices if p["priority"] == "high")
            high_priority_implemented = sum(1 for p in practices if p["priority"] == "high" and p["implemented"])

            assessment[category] = {
                "total_practices": total_practices,
                "implemented_practices": implemented_practices,
                "implementation_rate": implemented_practices / total_practices if total_practices > 0 else 0,
                "high_priority_implementation_rate": high_priority_implemented / high_priority_total if high_priority_total > 0 else 0,
                "missing_practices": [p["practice"] for p in practices if not p["implemented"]]
            }

        return assessment

# ä½¿ç”¨ç¤ºä¾‹
def development_workflow_demo():
    """å¼€å‘å·¥ä½œæµæ¼”ç¤º"""
    workflow = DevelopmentWorkflow()
    best_practice_checker = BestPracticeChecker()

    print("=== Agentic AIå¼€å‘æœ€ä½³å®è·µå·¥ä½œæµ ===")

    # è·å–å½“å‰é˜¶æ®µæŒ‡å¯¼
    current_stage = workflow.get_current_stage_guidance()
    if current_stage:
        print(f"\nğŸ“ å½“å‰é˜¶æ®µ: {current_stage.name}")
        print(f"æè¿°: {current_stage.description}")
        print(f"é¢„è®¡æ—¶é—´: {current_stage.estimated_time}")

        print("\nå…³é”®æ´»åŠ¨:")
        for activity in current_stage.key_activities:
            print(f"  - {activity}")

        print("\næˆåŠŸæ ‡å‡†:")
        for criterion in current_stage.success_criteria:
            print(f"  âœ“ {criterion}")

    # ç”Ÿæˆæ£€æŸ¥æ¸…å•
    checklist = workflow.generate_stage_checklist(current_stage)
    print(f"\nğŸ“‹ é˜¶æ®µæ£€æŸ¥æ¸…å• ({len(checklist)} é¡¹):")
    for item in checklist[:5]:  # æ˜¾ç¤ºå‰5é¡¹
        print(f"  {item['item_id']}. {item.get('activity', item.get('criterion', ''))}")

    # æœ€ä½³å®è·µæ£€æŸ¥
    bp_checklist = best_practice_checker.generate_checklist()
    print(f"\nğŸ¯ æœ€ä½³å®è·µæ£€æŸ¥:")
    for category, practices in bp_checklist.items():
        print(f"\n{category.upper()} ({len(practices)} é¡¹):")
        for practice in practices[:3]:  # æ˜¾ç¤ºå‰3é¡¹
            print(f"  - {practice['practice']} ({practice['priority']} priority)")

if __name__ == "__main__":
    development_workflow_demo()
```

---

## æœ¬ç« å°ç»“

### ğŸ¯ æ ¸å¿ƒè¦ç‚¹å›é¡¾

1. **è¯„ä¼°æ˜¯æ ¸å¿ƒèƒ½åŠ›**
   - æ²¡æœ‰è¯„ä¼°å°±æ— æ³•æ”¹è¿›
   - å®¢è§‚è¯„ä¼°å’Œä¸»è§‚è¯„ä¼°éœ€è¦ä¸åŒæ–¹æ³•
   - ç»„ä»¶çº§è¯„ä¼°æ¯”ç«¯åˆ°ç«¯è¯„ä¼°æ›´é«˜æ•ˆ

2. **é”™è¯¯åˆ†ææä¾›æ–¹å‘**
   - ç³»ç»ŸåŒ–çš„é”™è¯¯åˆ†ç±»å’Œç»Ÿè®¡
   - åŸºäºæ•°æ®ç¡®å®šä¼˜å…ˆçº§
   - é¿å…å‡­æ„Ÿè§‰åšå†³ç­–

3. **æŒç»­ä¼˜åŒ–æ˜¯å…³é”®**
   - ä»å¿«é€ŸåŸå‹å¼€å§‹
   - é€æ­¥å»ºç«‹è¯„ä¼°ä½“ç³»
   - åŸºäºæ•°æ®æŒç»­æ”¹è¿›

4. **æ€§èƒ½ä¸æˆæœ¬å¹³è¡¡**
   - è´¨é‡ > å»¶è¿Ÿ > æˆæœ¬
   - ç³»ç»ŸåŒ–çš„æ€§èƒ½ç›‘æ§
   - æœ‰é’ˆå¯¹æ€§çš„ä¼˜åŒ–ç­–ç•¥

### ğŸš€ å®è·µå»ºè®®

1. **ä»ä»Šå¤©å¼€å§‹è¯„ä¼°**
   - ä¸ºä½ çš„ç³»ç»Ÿå»ºç«‹åŸºç¡€è¯„ä¼°
   - æ”¶é›†10-20ä¸ªæµ‹è¯•æ¡ˆä¾‹
   - å»ºç«‹åŸºçº¿æŒ‡æ ‡

2. **å»ºç«‹é”™è¯¯è¿½è¸ªæœºåˆ¶**
   - è®°å½•æ‰€æœ‰é”™è¯¯å’Œå¤±è´¥æ¡ˆä¾‹
   - å®šæœŸåˆ†æé”™è¯¯æ¨¡å¼
   - åŸºäºæ•°æ®åˆ¶å®šæ”¹è¿›è®¡åˆ’

3. **é€æ­¥ä¼˜åŒ–**
   - ä¸€æ¬¡åªæ”¹å˜ä¸€ä¸ªå˜é‡
   - éªŒè¯æ¯ä¸ªæ”¹å˜çš„æ•ˆæœ
   - ä¿æŒæ”¹è¿›çš„è¿ç»­æ€§

4. **å…³æ³¨ç”¨æˆ·ä½“éªŒ**
   - å»¶è¿Ÿä¼˜åŒ–å¾€å¾€æ¯”æˆæœ¬ä¼˜åŒ–æ›´é‡è¦
   - åœ¨è´¨é‡å’Œæ•ˆç‡ä¹‹é—´æ‰¾åˆ°å¹³è¡¡
   - æŒç»­ç›‘æ§ç”¨æˆ·æ»¡æ„åº¦

### ğŸ“š ä¸‹ä¸€æ­¥å­¦ä¹ 

- **ç¬¬5ç« **ï¼šå­¦ä¹ æ„å»ºé«˜åº¦è‡ªæ²»çš„æ™ºèƒ½ä½“ç³»ç»Ÿ
- **æ·±å…¥å®è·µ**ï¼šå°†è¯„ä¼°æ–¹æ³•åº”ç”¨åˆ°å®é™…é¡¹ç›®ä¸­
- **é«˜çº§ä¸»é¢˜**ï¼šæ¢ç´¢æ›´å¤æ‚çš„è¯„ä¼°æŒ‡æ ‡å’Œä¼˜åŒ–ç­–ç•¥

---

## 8. å­¦ä¹ è·¯å¾„å»ºè®®

### 8.1 å®Œæ•´å­¦ä¹ è·¯å¾„ï¼ˆæ¨èï¼‰

å¦‚æœä½ æ˜¯ä»å¤´å¼€å§‹å­¦ä¹  Agentic AIï¼Œå»ºè®®æŒ‰ç…§ä»¥ä¸‹é¡ºåºï¼š

```
ç¬¬1ç« ï¼šAgentic å·¥ä½œæµç®€ä»‹
    â†“
ç¬¬2ç« ï¼šåæ€è®¾è®¡æ¨¡å¼å®è·µ
    â†“
ç¬¬3ç« ï¼šå·¥å…·ä½¿ç”¨å®æˆ˜
    â†“
ç¬¬4ç« ï¼šæ„å»ºAgentic AIçš„å®ç”¨æŠ€å·§ï¼ˆæœ¬ç« ï¼‰
    â†“
ç¬¬5ç« ï¼šæ„å»ºé«˜åº¦è‡ªæ²»çš„æ™ºèƒ½ä½“ç³»ç»Ÿ
```

**æ¯ç« æ ¸å¿ƒæ”¶è·**ï¼š
- **ç¬¬1ç« **ï¼šç†è§£åŸºç¡€æ¦‚å¿µï¼Œæ„å»ºç¬¬ä¸€ä¸ªå·¥ä½œæµ
- **ç¬¬2ç« **ï¼šæŒæ¡è‡ªæˆ‘æ”¹è¿›çš„åæ€æ¨¡å¼
- **ç¬¬3ç« **ï¼šå­¦ä¼šè®©AIè°ƒç”¨å¤–éƒ¨å·¥å…·
- **ç¬¬4ç« **ï¼šæŒæ¡è¯„ä¼°ã€ä¼˜åŒ–å’Œå¼€å‘æµç¨‹ï¼ˆæœ¬ç« ï¼‰
- **ç¬¬5ç« **ï¼šæ„å»ºå®Œå…¨è‡ªä¸»çš„æ™ºèƒ½ä½“ç³»ç»Ÿ

### 8.2 å¿«é€Ÿå®è·µè·¯å¾„ï¼ˆå·²æœ‰åŸºç¡€ï¼‰

å¦‚æœä½ å·²ç»æœ‰ AI å¼€å‘ç»éªŒï¼Œå¯ä»¥ï¼š

1. **ç›´æ¥å­¦ä¹ ç¬¬4ç« **ï¼ˆæœ¬ç« ï¼‰
   - ä½¿ç”¨"å‰ç½®çŸ¥è¯†å›é¡¾"å¿«é€Ÿäº†è§£åŸºç¡€æ¦‚å¿µ
   - è¿è¡Œ"å¿«é€Ÿå…¥é—¨ç¤ºä¾‹"ç«‹å³ä¸Šæ‰‹
   - é‡ç‚¹å­¦ä¹ è¯„ä¼°ä½“ç³»å’Œä¼˜åŒ–æ–¹æ³•

2. **é€‰æ‹©æ€§å­¦ä¹ å‰3ç« **
   - å¦‚æœä¸ç†Ÿæ‚‰"åæ€æ¨¡å¼"ï¼šå­¦ä¹ ç¬¬2ç« 
   - å¦‚æœä¸ç†Ÿæ‚‰"å·¥å…·ä½¿ç”¨"ï¼šå­¦ä¹ ç¬¬3ç« 
   - å¦‚æœåŸºç¡€æ¦‚å¿µä¸æ¸…æ¥šï¼šå­¦ä¹ ç¬¬1ç« 

### 8.3 é¡¹ç›®é©±åŠ¨å­¦ä¹ è·¯å¾„

æ ¹æ®ä½ çš„é¡¹ç›®éœ€æ±‚é€‰æ‹©å­¦ä¹ é‡ç‚¹ï¼š

| é¡¹ç›®ç±»å‹ | å»ºè®®å­¦ä¹ é‡ç‚¹ | å¯¹åº”ç« èŠ‚ |
|----------|--------------|----------|
| **è´¨é‡æå‡é¡¹ç›®** | è¯„ä¼°ä½“ç³»ã€é”™è¯¯åˆ†æã€ç»„ä»¶è¯„ä¼° | ç¬¬4ç« ï¼ˆå…¨éƒ¨ï¼‰ |
| **æ€§èƒ½ä¼˜åŒ–é¡¹ç›®** | å»¶è¿Ÿä¼˜åŒ–ã€æˆæœ¬ä¼˜åŒ–ã€æ€§èƒ½ç›‘æ§ | ç¬¬4ç« ç¬¬6èŠ‚ |
| **æ–°åŠŸèƒ½å¼€å‘** | å·¥ä½œæµè®¾è®¡ã€å·¥å…·é›†æˆ | ç¬¬1ã€3ç«  |
| **ç³»ç»Ÿé‡æ„** | å¼€å‘æµç¨‹ã€æœ€ä½³å®è·µ | ç¬¬4ç« ç¬¬7èŠ‚ |

### 8.4 å­¦ä¹ æ—¶é—´è§„åˆ’

| å­¦ä¹ æ¨¡å¼ | é¢„è®¡æ—¶é—´ | å­¦ä¹ å†…å®¹ |
|----------|----------|----------|
| **å¿«é€Ÿæµè§ˆ** | 2-3å°æ—¶ | é˜…è¯»æ ¸å¿ƒæ¦‚å¿µï¼Œè¿è¡Œå¿«é€Ÿç¤ºä¾‹ |
| **ç³»ç»Ÿå­¦ä¹ ** | 8-10å°æ—¶ | å®Œæˆæ‰€æœ‰ç¤ºä¾‹ä»£ç å’Œå®æˆ˜é¡¹ç›® |
| **æ·±åº¦å®è·µ** | 20+å°æ—¶ | å°†æ–¹æ³•åº”ç”¨åˆ°å®é™…é¡¹ç›®ï¼Œè¿­ä»£ä¼˜åŒ– |

### 8.5 æ£€æŸ¥å­¦ä¹ æ•ˆæœ

å®Œæˆæœ¬ç« å­¦ä¹ åï¼Œä½ åº”è¯¥èƒ½å¤Ÿï¼š

âœ… **åŸºç¡€æŒæ¡**ï¼š
- ä¸ºAIç³»ç»Ÿå»ºç«‹åŸºç¡€è¯„ä¼°ä½“ç³»
- è¯†åˆ«å’Œåˆ†ç±»å¸¸è§é”™è¯¯ç±»å‹
- ç†è§£æ€§èƒ½ä¼˜åŒ–çš„åŸºæœ¬æ–¹æ³•

âœ… **ä¸­çº§åº”ç”¨**ï¼š
- å®æ–½ç»„ä»¶çº§è¯„ä¼°
- åŸºäºé”™è¯¯åˆ†æåˆ¶å®šæ”¹è¿›è®¡åˆ’
- å¹³è¡¡è´¨é‡ã€å»¶è¿Ÿå’Œæˆæœ¬

âœ… **é«˜çº§å®è·µ**ï¼š
- è®¾è®¡å®Œæ•´çš„å¼€å‘å·¥ä½œæµ
- å»ºç«‹æŒç»­æ”¹è¿›æœºåˆ¶
- å°†è¯„ä¼°æ–¹æ³•åº”ç”¨åˆ°å¤æ‚é¡¹ç›®

**ä¸‹ä¸€æ­¥è¡ŒåŠ¨å»ºè®®**ï¼š
1. ç«‹å³ä¸ºä½ çš„é¡¹ç›®å»ºç«‹10ä¸ªæµ‹è¯•æ¡ˆä¾‹
2. è¿è¡Œä¸€æ¬¡åŸºç¡€è¯„ä¼°ï¼Œå»ºç«‹åŸºçº¿æŒ‡æ ‡
3. é€‰æ‹©ä¸€ä¸ªæœ€å½±å“ç”¨æˆ·ä½“éªŒçš„é—®é¢˜è¿›è¡Œä¼˜åŒ–

---

**æ­å–œä½ å®Œæˆç¬¬4ç« å­¦ä¹ ï¼** ğŸ‰

ä½ å·²ç»æŒæ¡äº†æ„å»ºé«˜è´¨é‡Agentic AIç³»ç»Ÿçš„æ ¸å¿ƒæ–¹æ³•è®ºï¼ŒåŒ…æ‹¬è¯„ä¼°ã€é”™è¯¯åˆ†æã€æ€§èƒ½ä¼˜åŒ–ç­‰å…³é”®æŠ€èƒ½ã€‚

**è®°ä½**ï¼šè¯„ä¼°æ˜¯æŒç»­æ”¹è¿›çš„åŸºç¡€ï¼Œæ•°æ®é©±åŠ¨å†³ç­–æ˜¯æˆåŠŸçš„å…³é”®ã€‚ç°åœ¨ä½ å¯ä»¥æ„å»ºä¸ä»…åŠŸèƒ½å¼ºå¤§ï¼Œè€Œä¸”è´¨é‡å¯é ã€æ€§èƒ½ä¼˜ç§€çš„AIç³»ç»Ÿäº†ï¼